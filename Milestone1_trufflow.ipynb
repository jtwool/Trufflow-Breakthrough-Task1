{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29401ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HEADER / TOP-LEVEL KEYS ===\n",
      "['tenant/id', 'transaction/consumer/id', 'transaction/consumer/name', 'transaction/cost', 'transaction/data', 'transaction/id', 'transaction/response', 'transaction/supplier/id', 'transaction/supplier/name', 'transaction/time']\n",
      "\n",
      "=== EXPECTED FIELD CHECK ===\n",
      "Present: []\n",
      "Missing: ['ts', 'timestamp', 'service_id', 'peer_id', 'req_count', 'error_count', 'p50_latency_ms', 'p95_latency_ms', 'bytes_in', 'bytes_out']\n",
      "\n",
      "=== FIRST 5 ROWS ===\n",
      "\n",
      "--- Row 1 ---\n",
      "{'tenant/id': 'DEMO', 'transaction/consumer/id': 'SELENE', 'transaction/consumer/name': 'Selene Customer Warehouse', 'transaction/cost': 41.5, 'transaction/data': 0.3, 'transaction/id': 't0x78b44bd3-GWVR7-O9HZF', 'transaction/response': 'success', 'transaction/supplier/id': 'AWS', 'transaction/supplier/name': 'Amazon Web Services', 'transaction/time': '2024-06-01 00:00:00'}\n",
      "\n",
      "--- Row 2 ---\n",
      "{'tenant/id': 'DEMO', 'transaction/consumer/id': 'SELENE', 'transaction/consumer/name': 'Selene Customer Warehouse', 'transaction/cost': 49.8, 'transaction/data': 0.45, 'transaction/id': 't0x78b44bd3-GWVR7-O9HZF', 'transaction/response': 'success', 'transaction/supplier/id': 'DBRCKS', 'transaction/supplier/name': 'Databricks', 'transaction/time': '2024-06-01 00:00:00'}\n",
      "\n",
      "--- Row 3 ---\n",
      "{'tenant/id': 'DEMO', 'transaction/consumer/id': 'SELENE', 'transaction/consumer/name': 'Selene Customer Warehouse', 'transaction/cost': 24.9, 'transaction/data': 0.9, 'transaction/id': 't0x78b44bd3-GWVR7-O9HZF', 'transaction/response': 'success', 'transaction/supplier/id': 'SNWFLK', 'transaction/supplier/name': 'Snowflake', 'transaction/time': '2024-06-01 00:00:00'}\n",
      "\n",
      "--- Row 4 ---\n",
      "{'tenant/id': 'DEMO', 'transaction/consumer/id': 'FRDETCT', 'transaction/consumer/name': 'Fraud Detection System', 'transaction/cost': 116.2, 'transaction/data': 1.65, 'transaction/id': 't0x78b44bd3-GWVR7-O9HZF', 'transaction/response': 'success', 'transaction/supplier/id': 'SELENE', 'transaction/supplier/name': 'Selene Customer Warehouse', 'transaction/time': '2024-06-01 00:00:00'}\n",
      "\n",
      "--- Row 5 ---\n",
      "{'tenant/id': 'DEMO', 'transaction/consumer/id': 'HMFRP', 'transaction/consumer/name': 'Hermes Fast Release Platform', 'transaction/cost': 16.6, 'transaction/data': 0.15, 'transaction/id': 't0x78b44bd3-GWVR7-O9HZF', 'transaction/response': 'success', 'transaction/supplier/id': 'AZURE', 'transaction/supplier/name': 'Microsoft Azure', 'transaction/time': '2024-06-01 00:00:00'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect transactions.jsonl (single cell)\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "EXPECTED = [\n",
    "    \"ts\",\"timestamp\",\"service_id\",\"peer_id\",\n",
    "    \"req_count\",\"error_count\",\"p50_latency_ms\",\"p95_latency_ms\",\n",
    "    \"bytes_in\",\"bytes_out\"\n",
    "]\n",
    "\n",
    "def inspect_jsonl(path: str, n: int = 5):\n",
    "    keys_union, samples = set(), []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in islice(f, n):\n",
    "            if not line.strip(): \n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            samples.append(obj)\n",
    "            keys_union.update(obj.keys())\n",
    "    keys_union = sorted(keys_union)\n",
    "    print(\"=== HEADER / TOP-LEVEL KEYS ===\")\n",
    "    print(keys_union)\n",
    "    print(\"\\n=== EXPECTED FIELD CHECK ===\")\n",
    "    present = [k for k in EXPECTED if k in keys_union]\n",
    "    missing = [k for k in EXPECTED if k not in keys_union]\n",
    "    print(\"Present:\", present)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(f\"\\n=== FIRST {len(samples)} ROWS ===\")\n",
    "    for i, obj in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Row {i} ---\")\n",
    "        print({k: obj.get(k) for k in keys_union})\n",
    "\n",
    "inspect_jsonl(\"transactions.jsonl\", n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efa0ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read from JSONL: 7254656\n",
      "Aggregated windows: 1540175\n",
      "Feature matrix shape: (1540175, 6)  # [samples, features]\n",
      "00 key=('SELENE', 'AWS', '2024-06-01 00:00:00')  feats=[71.0, 0.0, 1630.9500000000007, 22.97112676056339, 25.650000000000002, 0.36126760563380284]\n",
      "01 key=('SELENE', 'DBRCKS', '2024-06-01 00:00:00')  feats=[71.0, 0.0, 3083.45, 43.42887323943662, 85.50000000000001, 1.2042253521126762]\n",
      "02 key=('SELENE', 'SNWFLK', '2024-06-01 00:00:00')  feats=[71.0, 0.0, 1618.500000000001, 22.795774647887338, 65.4, 0.9211267605633804]\n",
      "03 key=('FRDETCT', 'SELENE', '2024-06-01 00:00:00')  feats=[1.0, 0.0, 116.2, 116.2, 1.65, 1.65]\n",
      "04 key=('HMFRP', 'AZURE', '2024-06-01 00:00:00')  feats=[71.0, 0.0, 1361.2, 19.171830985915495, 8.250000000000002, 0.11619718309859157]\n"
     ]
    }
   ],
   "source": [
    "# Build hourly features from transactions.jsonl (single cell)\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# --- helpers ---\n",
    "def floor_to_hour(ts: str):\n",
    "    \"\"\"Parse timestamp and floor to the hour, return 'YYYY-MM-DD HH:00:00' or None.\"\"\"\n",
    "    if not ts:\n",
    "        return None\n",
    "    fmts = [\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%Y-%m-%dT%H:%M:%S\",\n",
    "        \"%Y-%m-%d %H:%M:%S.%f\",\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f\",\n",
    "    ]\n",
    "    for fmt in fmts:\n",
    "        try:\n",
    "            dt = datetime.strptime(ts, fmt)\n",
    "            dt = dt.replace(minute=0, second=0, microsecond=0)\n",
    "            return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def is_success(resp):\n",
    "    \"\"\"Return True if response indicates success.\"\"\"\n",
    "    if resp is None:\n",
    "        return False\n",
    "    s = str(resp).strip().lower()\n",
    "    return s in {\"success\", \"ok\", \"200\", \"true\", \"passed\"}\n",
    "\n",
    "# --- aggregate per (consumer_id, supplier_id, hour) ---\n",
    "agg = defaultdict(lambda: {\n",
    "    \"n\": 0,\n",
    "    \"success\": 0,\n",
    "    \"cost_sum\": 0.0,\n",
    "    \"cost_sqsum\": 0.0,   # optional variance later\n",
    "    \"data_sum\": 0.0,\n",
    "    \"data_sqsum\": 0.0,   # optional variance later\n",
    "})\n",
    "\n",
    "path = \"transactions.jsonl\"  # adjust if needed\n",
    "rows_read = 0\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        ts = obj.get(\"transaction/time\")\n",
    "        bucket = floor_to_hour(ts)\n",
    "        if bucket is None:\n",
    "            continue\n",
    "\n",
    "        consumer = obj.get(\"transaction/consumer/id\") or \"\"\n",
    "        supplier = obj.get(\"transaction/supplier/id\") or \"\"\n",
    "        if not consumer or not supplier:\n",
    "            continue\n",
    "\n",
    "        cost = obj.get(\"transaction/cost\", 0.0) or 0.0\n",
    "        data = obj.get(\"transaction/data\", 0.0) or 0.0\n",
    "        resp = obj.get(\"transaction/response\")\n",
    "\n",
    "        key = (consumer, supplier, bucket)\n",
    "        a = agg[key]\n",
    "        a[\"n\"] += 1\n",
    "        a[\"success\"] += 1 if is_success(resp) else 0\n",
    "        a[\"cost_sum\"] += float(cost)\n",
    "        a[\"cost_sqsum\"] += float(cost) ** 2\n",
    "        a[\"data_sum\"] += float(data)\n",
    "        a[\"data_sqsum\"] += float(data) ** 2\n",
    "        rows_read += 1\n",
    "\n",
    "# --- build feature matrix ---\n",
    "keys = []\n",
    "rows = []\n",
    "for key, a in agg.items():\n",
    "    n = float(a[\"n\"])\n",
    "    succ = float(a[\"success\"])\n",
    "    err = max(n - succ, 0.0)\n",
    "    err_rate = (err / n) if n > 0 else 0.0\n",
    "\n",
    "    cost_sum = a[\"cost_sum\"]\n",
    "    data_sum = a[\"data_sum\"]\n",
    "    cost_mean = (cost_sum / n) if n > 0 else 0.0\n",
    "    data_mean = (data_sum / n) if n > 0 else 0.0\n",
    "\n",
    "    # features: [req_count, error_rate, cost_sum, cost_mean, data_sum, data_mean]\n",
    "    feat = [n, err_rate, cost_sum, cost_mean, data_sum, data_mean]\n",
    "    rows.append(feat)\n",
    "    keys.append(key)\n",
    "\n",
    "X = np.array(rows, dtype=float) if rows else np.zeros((0, 6), dtype=float)\n",
    "\n",
    "print(f\"Rows read from JSONL: {rows_read}\")\n",
    "print(f\"Aggregated windows: {len(keys)}\")\n",
    "print(f\"Feature matrix shape: {X.shape}  # [samples, features]\")\n",
    "\n",
    "# show a small preview\n",
    "for i in range(min(5, len(keys))):\n",
    "    (consumer, supplier, bucket) = keys[i]\n",
    "    print(f\"{i:02d} key={keys[i]}  feats={X[i].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e969cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_size': 308035, 'pos_rate_valid': 0.0848767185547097, 'PR_AUC': 0.9774733165108653, 'F1': 0.9235196292766784, 'Precision': 0.9028533849694933, 'Recall': 0.9451902849493211, 'Acc_pos': 0.9451902849493211, 'Acc_mean': 0.9867125488986641, 'threshold': 0.9806067807015026}\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Naive Bayes baseline (weak labels + metrics)\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score, accuracy_score\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# Features: [req_count, error_rate, cost_sum, cost_mean, data_sum, data_mean]\n",
    "er = X[:, 1]\n",
    "cmean = X[:, 3]\n",
    "dmean = X[:, 5]\n",
    "\n",
    "def robust_z(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "\n",
    "z_c = robust_z(cmean)\n",
    "z_d = robust_z(dmean)\n",
    "\n",
    "# Weak labels: anomaly if high error_rate OR cost/data mean outlier\n",
    "y = ((er > 0.10) | (z_c > 3.0) | (z_d > 3.0)).astype(int)\n",
    "\n",
    "# Time-based split (80/20) by hour bucket string\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx) * 0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:] if cut > 0 else (idx, idx)\n",
    "\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"nb\", GaussianNB())\n",
    "]).fit(Xtr, ytr)\n",
    "\n",
    "if len(Xva) and len(np.unique(yva)) > 1:\n",
    "    proba = model.predict_proba(Xva)[:, 1]\n",
    "    pr_auc = average_precision_score(yva, proba)\n",
    "    prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "    f1s = (2 * prec * rec) / (prec + rec + 1e-9)\n",
    "    bi = int(np.argmax(f1s))\n",
    "    best_thr = thr[bi-1] if bi > 0 and (bi-1) < len(thr) else 0.5\n",
    "    yhat = (proba >= best_thr).astype(int)\n",
    "    acc_pos = accuracy_score(yva[yva == 1], yhat[yva == 1]) if np.any(yva == 1) else 0.0\n",
    "    acc_mean = accuracy_score(yva, yhat)\n",
    "    print({\n",
    "        \"valid_size\": int(len(yva)),\n",
    "        \"pos_rate_valid\": float(yva.mean()),\n",
    "        \"PR_AUC\": float(pr_auc),\n",
    "        \"F1\": float(f1_score(yva, yhat)),\n",
    "        \"Precision\": float(prec[bi]),\n",
    "        \"Recall\": float(rec[bi]),\n",
    "        \"Acc_pos\": float(acc_pos),\n",
    "        \"Acc_mean\": float(acc_mean),\n",
    "        \"threshold\": float(best_thr)\n",
    "    })\n",
    "else:\n",
    "    print({\"note\": \"Validation split has only one class; try adjusting weak labels or split.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be3260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task1_results.csv and task1_results.json\n",
      "{'Feature Set / Variant': 'All features (NB)', 'Precision': 0.9028533849694933, 'Recall': 0.9451902849493211, 'F1': 0.9235196292766784, 'PR-AUC': 0.9774733165108653, 'Accuracy (+ve)': 0.9451902849493211, 'Accuracy (mean)': 0.9867125488986641, 'Threshold': 0.9806067807015026, 'Valid size': 308035, 'Pos rate (valid)': 0.0848767185547097}\n"
     ]
    }
   ],
   "source": [
    "# Save Task 1 metrics to CSV + JSON (one-row table)\n",
    "import csv, json\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run feature cell first.\"\n",
    "\n",
    "# Rebuild weak labels (same as before)\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "z_c = robust_z(cmean); z_d = robust_z(dmean)\n",
    "y = ((er > 0.10) | (z_c > 3.0) | (z_d > 3.0)).astype(int)\n",
    "\n",
    "# Time split\n",
    "idx = np.arange(len(keys)); idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "# Train NB\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]).fit(Xtr, ytr)\n",
    "\n",
    "# Metrics\n",
    "proba = model.predict_proba(Xva)[:,1]\n",
    "pr_auc = average_precision_score(yva, proba)\n",
    "prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "bi = int(np.argmax(f1s))\n",
    "best_thr = thr[bi-1] if bi>0 and (bi-1)<len(thr) else 0.5\n",
    "yhat = (proba >= best_thr).astype(int)\n",
    "acc_pos = accuracy_score(yva[yva==1], yhat[yva==1]) if np.any(yva==1) else 0.0\n",
    "acc_mean = accuracy_score(yva, yhat)\n",
    "row = {\n",
    "    \"Feature Set / Variant\": \"All features (NB)\",\n",
    "    \"Precision\": float(prec[bi]),\n",
    "    \"Recall\": float(rec[bi]),\n",
    "    \"F1\": float(f1_score(yva, yhat)),\n",
    "    \"PR-AUC\": float(pr_auc),\n",
    "    \"Accuracy (+ve)\": float(acc_pos),\n",
    "    \"Accuracy (mean)\": float(acc_mean),\n",
    "    \"Threshold\": float(best_thr),\n",
    "    \"Valid size\": int(len(yva)),\n",
    "    \"Pos rate (valid)\": float(yva.mean())\n",
    "}\n",
    "\n",
    "# Write CSV\n",
    "csv_path = \"task1_results.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "    w.writeheader(); w.writerow(row)\n",
    "\n",
    "# Write JSON\n",
    "json_path = \"task1_results.json\"\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"results\": [row]}, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", csv_path, \"and\", json_path)\n",
    "print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c77986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Services: 87 Vector shape: (87, 8)\n",
      "ABTEST -> [('NOTIFY', 0.9999999999809024), ('MKTDB', 0.9999999999349333), ('DYNPRC', 0.9999999998786389), ('XCHATTR', 0.9999999997867001), ('TAXCALC', 0.9999999996770227)]\n",
      "AIRFLW -> [('HMFRP', 0.999999998609385), ('YELLOWS', 0.9999999953721767), ('KAFKA', 0.9999999888122127), ('DDSD', 0.9999999793005887), ('ELASTIC', 0.9999996950612356)]\n",
      "ANALAPI -> [('AUTOML', 0.9999992614814076), ('RISKMG', 0.9999991143549807), ('BILLING', 0.9999989627174771), ('GLOBCMP', 0.9999989224119094), ('SUPCHN', 0.9999988448202543)]\n",
      "APIGWY -> [('SNAPINT', 0.9999999992863867), ('MULTILNG', 0.9999999971777137), ('SLSDB', 0.9999999962098263), ('TAXCALC', 0.9999999958172383), ('XCHATTR', 0.9999999955270946)]\n",
      "ATLAS -> [('DATAHUB', 0.9999896440875172), ('MDLREG', 0.9999718818442569), ('JPNXPP', 0.999951212040954), ('SPARK', 0.9999474668239722), ('PANDRA', 0.9999318254451391)]\n",
      "Saved task2_neighbors.json\n"
     ]
    }
   ],
   "source": [
    "# Task 2 — Service similarity (cosine over consumer/supplier profiles)\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "k = 5  # top-k neighbors\n",
    "\n",
    "# Accumulate per service, separately for consumer and supplier roles.\n",
    "# For each service we keep:\n",
    "# [n_cons, err_wsum_cons, cost_wsum_cons, data_wsum_cons,\n",
    "#  n_sup,  err_wsum_sup,  cost_wsum_sup,  data_wsum_sup]\n",
    "acc = defaultdict(lambda: np.zeros(8, dtype=float))\n",
    "\n",
    "for (cons, supp, _), row in zip(keys, X):\n",
    "    n, err_rate, _cost_sum, cost_mean, _data_sum, data_mean = row\n",
    "\n",
    "    # consumer role\n",
    "    a = acc[cons]\n",
    "    a[0] += n\n",
    "    a[1] += err_rate * n\n",
    "    a[2] += cost_mean * n\n",
    "    a[3] += data_mean * n\n",
    "\n",
    "    # supplier role\n",
    "    b = acc[supp]\n",
    "    b[4] += n\n",
    "    b[5] += err_rate * n\n",
    "    b[6] += cost_mean * n\n",
    "    b[7] += data_mean * n\n",
    "\n",
    "services = sorted(acc.keys())\n",
    "mat = np.zeros((len(services), 8), dtype=float)\n",
    "\n",
    "# Convert weighted sums to means where appropriate\n",
    "for i, s in enumerate(services):\n",
    "    v = acc[s].copy()\n",
    "    # consumer means\n",
    "    v[1] = v[1] / (v[0] + 1e-9)\n",
    "    v[2] = v[2] / (v[0] + 1e-9)\n",
    "    v[3] = v[3] / (v[0] + 1e-9)\n",
    "    # supplier means\n",
    "    v[5] = v[5] / (v[4] + 1e-9)\n",
    "    v[6] = v[6] / (v[4] + 1e-9)\n",
    "    v[7] = v[7] / (v[4] + 1e-9)\n",
    "    mat[i] = v\n",
    "\n",
    "# Cosine similarity on normalized vectors\n",
    "Xn = normalize(mat)\n",
    "S = cosine_similarity(Xn)\n",
    "\n",
    "neighbors = {}\n",
    "for i, s in enumerate(services):\n",
    "    order = np.argsort(-S[i])\n",
    "    top = [(services[j], float(S[i, j])) for j in order[1:k+1]]  # skip self\n",
    "    neighbors[s] = top\n",
    "\n",
    "print(\"Services:\", len(services), \"Vector shape:\", mat.shape)\n",
    "for i in range(min(5, len(services))):\n",
    "    print(services[i], \"->\", neighbors[services[i]][:k])\n",
    "\n",
    "with open(\"task2_neighbors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"k\": k, \"neighbors\": neighbors}, f, indent=2)\n",
    "print(\"Saved task2_neighbors.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d121fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task2_neighbors.csv\n",
      "Saved: task2_clustering_results.csv\n",
      "Best k: 8  Silhouette(cosine): 0.9835  -> task2_clusters_k8.csv\n"
     ]
    }
   ],
   "source": [
    "# Task 2 tables: neighbors CSV + KMeans clustering (silhouette) summary\n",
    "import json, csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "assert 'services' in globals() and 'mat' in globals(), \"Run the Task 2 cell first.\"\n",
    "\n",
    "# 1) Neighbors CSV (top-5)\n",
    "Xn = normalize(mat)\n",
    "S = cosine_similarity(Xn)\n",
    "\n",
    "k = 5\n",
    "rows = []\n",
    "for i, s in enumerate(services):\n",
    "    order = np.argsort(-S[i])\n",
    "    nbrs = [(services[j], float(S[i,j])) for j in order[1:k+1]]\n",
    "    rows.append({\n",
    "        \"service\": s,\n",
    "        \"n1\": nbrs[0][0], \"s1\": nbrs[0][1],\n",
    "        \"n2\": nbrs[1][0], \"s2\": nbrs[1][1],\n",
    "        \"n3\": nbrs[2][0], \"s3\": nbrs[2][1],\n",
    "        \"n4\": nbrs[3][0], \"s4\": nbrs[3][1],\n",
    "        \"n5\": nbrs[4][0], \"s5\": nbrs[4][1],\n",
    "    })\n",
    "\n",
    "with open(\"task2_neighbors.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fieldnames = [\"service\",\"n1\",\"s1\",\"n2\",\"s2\",\"n3\",\"s3\",\"n4\",\"s4\",\"n5\",\"s5\"]\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in rows: w.writerow(r)\n",
    "\n",
    "# 2) Clustering summary (silhouette across k)\n",
    "ks = [3,4,5,6,7,8,9,10]\n",
    "summary = []\n",
    "best = (None, -1.0, None)  # (k, score, labels)\n",
    "\n",
    "for k_ in ks:\n",
    "    km = KMeans(n_clusters=k_, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(Xn)\n",
    "    sil = silhouette_score(Xn, labels, metric=\"cosine\")\n",
    "    summary.append({\"Representation\":\"Role means (8-dim)\", \"Method\":\"KMeans\", \"k\":k_, \"Silhouette\":float(sil)})\n",
    "    if sil > best[1]:\n",
    "        best = (k_, sil, labels)\n",
    "\n",
    "with open(\"task2_clustering_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fieldnames = [\"Representation\",\"Method\",\"k\",\"Silhouette\"]\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in summary: w.writerow(r)\n",
    "\n",
    "# Save best-k cluster assignments\n",
    "best_k, best_sil, best_labels = best\n",
    "with open(f\"task2_clusters_k{best_k}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"service\",\"cluster\"])\n",
    "    for s, lbl in zip(services, best_labels):\n",
    "        w.writerow([s, int(lbl)])\n",
    "\n",
    "print(\"Saved: task2_neighbors.csv\")\n",
    "print(\"Saved: task2_clustering_results.csv\")\n",
    "print(f\"Best k: {best_k}  Silhouette(cosine): {best_sil:.4f}  -> task2_clusters_k{best_k}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabf2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task2_neighbors.csv\n",
      "Saved: task2_clustering_results.csv\n",
      "Best k: 8  Silhouette(cosine): 0.9835  -> task2_clusters_k8.csv\n"
     ]
    }
   ],
   "source": [
    "# Task 2 tables: neighbors CSV + KMeans clustering (silhouette) summary\n",
    "import json, csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "assert 'services' in globals() and 'mat' in globals(), \"Run the Task 2 cell first.\"\n",
    "\n",
    "# 1) Neighbors CSV (top-5)\n",
    "Xn = normalize(mat)\n",
    "S = cosine_similarity(Xn)\n",
    "\n",
    "k = 5\n",
    "rows = []\n",
    "for i, s in enumerate(services):\n",
    "    order = np.argsort(-S[i])\n",
    "    nbrs = [(services[j], float(S[i,j])) for j in order[1:k+1]]\n",
    "    rows.append({\n",
    "        \"service\": s,\n",
    "        \"n1\": nbrs[0][0], \"s1\": nbrs[0][1],\n",
    "        \"n2\": nbrs[1][0], \"s2\": nbrs[1][1],\n",
    "        \"n3\": nbrs[2][0], \"s3\": nbrs[2][1],\n",
    "        \"n4\": nbrs[3][0], \"s4\": nbrs[3][1],\n",
    "        \"n5\": nbrs[4][0], \"s5\": nbrs[4][1],\n",
    "    })\n",
    "\n",
    "with open(\"task2_neighbors.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fieldnames = [\"service\",\"n1\",\"s1\",\"n2\",\"s2\",\"n3\",\"s3\",\"n4\",\"s4\",\"n5\",\"s5\"]\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in rows: w.writerow(r)\n",
    "\n",
    "# 2) Clustering summary (silhouette across k)\n",
    "ks = [3,4,5,6,7,8,9,10]\n",
    "summary = []\n",
    "best = (None, -1.0, None)  # (k, score, labels)\n",
    "\n",
    "for k_ in ks:\n",
    "    km = KMeans(n_clusters=k_, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(Xn)\n",
    "    sil = silhouette_score(Xn, labels, metric=\"cosine\")\n",
    "    summary.append({\"Representation\":\"Role means (8-dim)\", \"Method\":\"KMeans\", \"k\":k_, \"Silhouette\":float(sil)})\n",
    "    if sil > best[1]:\n",
    "        best = (k_, sil, labels)\n",
    "\n",
    "with open(\"task2_clustering_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fieldnames = [\"Representation\",\"Method\",\"k\",\"Silhouette\"]\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in summary: w.writerow(r)\n",
    "\n",
    "# Save best-k cluster assignments\n",
    "best_k, best_sil, best_labels = best\n",
    "with open(f\"task2_clusters_k{best_k}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"service\",\"cluster\"])\n",
    "    for s, lbl in zip(services, best_labels):\n",
    "        w.writerow([s, int(lbl)])\n",
    "\n",
    "print(\"Saved: task2_neighbors.csv\")\n",
    "print(\"Saved: task2_clustering_results.csv\")\n",
    "print(f\"Best k: {best_k}  Silhouette(cosine): {best_sil:.4f}  -> task2_clusters_k{best_k}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f831aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tables_for_report.md\n",
      "# Results Tables\n",
      "\n",
      "## Task 1 — Insight Detection (Naive Bayes Baseline)\n",
      "\n",
      "| Feature Set / Variant | Precision | Recall | F1 | PR-AUC | Accuracy (+ve) | Accuracy (mean) | Threshold | Valid size | Pos rate (valid) |\n",
      "|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n",
      "| All features (NB) | 0.9028533849694933 | 0.9451902849493211 | 0.9235196292766784 | 0.9774733165108653 | 0.9451902849493211 | 0.9867125488986641 | 0.9806067807015026 | 308035 | 0.0848767185547097 |\n",
      "\n",
      "## Task 2 — Service Similarity (Top-5 Neighbors, sample)\n",
      "| service | n1 | s1 | n2 | s2 | n3 | s3 | n4 | s4 | n5 | s5 |\n",
      "|---|---|---:|---|---:|---|---:|---|---:|---|---:|\n",
      "| ABTEST | NOTIFY | 0.9999999999809024 | MKTDB | 0.9999999999349333 | DYNPRC | 0.9999999998786389 | XCHATTR | 0.9999999997867001 | TAXCALC | 0.9999999996770227 |\n",
      "| AIRFLW | HMFRP | 0.999999998609385 | YELLOWS | 0.9999999953721767 | KAFKA | 0.9999999888122127 | DDSD | 0.9999999793005887 | ELASTIC | 0.9999996950612356 |\n",
      "| ANALAPI | AUTOML | 0.9999992614814076 | RISKMG | 0.9999991143549807 | BILLING | 0.9999989627174771 | GLOBCMP | 0.9999989224119094 | SUPCHN | 0.9999988448202543 |\n",
      "| APIGWY | SNAPINT | 0.9999999992863867 | MULTILNG | 0.9999999971777137 | SLSDB | 0.9999999962098263 | TAXCALC | 0.9999999958172383 | XCHATTR | 0.9999999955270946 |\n",
      "| ATLAS | DATAHUB | 0.9999896440875172 | MDLREG | 0.9999718818442569 | JPNXPP | 0.999951212040954 | SPARK | 0.9999474668239722 | PANDRA | 0.9999318254451391 |\n",
      "| AUTOML | RISKMG | 0.9999999930268135 | BILLING | 0.9999999744404572 | GLOBCMP | 0.9999999675906348 | SUPCHN | 0.9999999532220397 | PAYPROC | 0.9999998582373713 |\n",
      "| AWS | AZURE | 0.9999999999941895 | SNWFLK | 0.9999999999270122 | DBRCKS | 0.9999999998043644 | HMFRP | 0.7071067811150845 | AIRFLW | 0.7071067796016148 |\n",
      "| AZURE | AWS | 0.9999999999941895 | SNWFLK | 0.9999999999622053 | DBRCKS | 0.9999999998659184 | HMFRP | 0.7071067811452451 | AIRFLW | 0.7071067797588978 |\n",
      "| BILLING | GLOBCMP | 0.999999999573766 | SUPCHN | 0.9999999968161862 | RISKMG | 0.9999999940065972 | AUTOML | 0.9999999744404572 | PAYPROC | 0.9999999528774104 |\n",
      "| CHURNP | FINPLAN | 0.9999999791190787 | INVOPT | 0.9999998271869271 | RTDASH | 0.9999997446540816 | GEOANL | 0.9999995496414685 | TRNPIPE | 0.9999972450839603 |\n",
      "\n",
      "## Task 2 — Clustering Summary (Silhouette, cosine)\n",
      "| Representation | Method | k | Silhouette |\n",
      "|---|---|---:|---:|\n",
      "| Role means (8-dim) | KMeans | 3 | 0.851158675092405 |\n",
      "| Role means (8-dim) | KMeans | 4 | 0.8687091680037328 |\n",
      "| Role means (8-dim) | KMeans | 5 | 0.933113627229332 |\n",
      "| Role means (8-dim) | KMeans | 6 | 0.9661969420545483 |\n",
      "| Role means (8-dim) | KMeans | 7 | 0.9631850236343544 |\n",
      "| Role means (8-dim) | KMeans | 8 | 0.9834557457986388 |\n",
      "| Role means (8-dim) | KMeans | 9 | 0.9743887732550746 |\n",
      "| Role means (8-dim) | KMeans | 10 | 0.9673060323981396 |\n"
     ]
    }
   ],
   "source": [
    "# Build a report-ready Markdown with Task 1 & Task 2 tables\n",
    "import csv, os, json\n",
    "\n",
    "task1_csv = \"task1_results.csv\"\n",
    "task2_neighbors_csv = \"task2_neighbors.csv\"\n",
    "task2_cluster_csv = \"task2_clustering_results.csv\"\n",
    "out_md = \"tables_for_report.md\"\n",
    "\n",
    "# --- load Task 1 (one row) ---\n",
    "with open(task1_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "    r = list(csv.DictReader(f))\n",
    "t1 = r[0] if r else {}\n",
    "\n",
    "# --- load Task 2 neighbors (limit to 10 services for the doc) ---\n",
    "rows_nbr = []\n",
    "if os.path.exists(task2_neighbors_csv):\n",
    "    with open(task2_neighbors_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows_nbr = list(csv.DictReader(f))\n",
    "rows_nbr = rows_nbr[:10]\n",
    "\n",
    "# --- load Task 2 clustering summary ---\n",
    "rows_clu = []\n",
    "if os.path.exists(task2_cluster_csv):\n",
    "    with open(task2_cluster_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows_clu = list(csv.DictReader(f))\n",
    "\n",
    "md = []\n",
    "\n",
    "md.append(\"# Results Tables\\n\")\n",
    "\n",
    "# Task 1 table\n",
    "md.append(\"## Task 1 — Insight Detection (Naive Bayes Baseline)\\n\")\n",
    "md.append(\"| Feature Set / Variant | Precision | Recall | F1 | PR-AUC | Accuracy (+ve) | Accuracy (mean) | Threshold | Valid size | Pos rate (valid) |\")\n",
    "md.append(\"|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\")\n",
    "md.append(f\"| {t1.get('Feature Set / Variant','')} | {t1.get('Precision','')} | {t1.get('Recall','')} | {t1.get('F1','')} | {t1.get('PR-AUC','')} | {t1.get('Accuracy (+ve)','')} | {t1.get('Accuracy (mean)','')} | {t1.get('Threshold','')} | {t1.get('Valid size','')} | {t1.get('Pos rate (valid)','')} |\")\n",
    "md.append(\"\")\n",
    "\n",
    "# Task 2 neighbors table (sample)\n",
    "md.append(\"## Task 2 — Service Similarity (Top-5 Neighbors, sample)\")\n",
    "md.append(\"| service | n1 | s1 | n2 | s2 | n3 | s3 | n4 | s4 | n5 | s5 |\")\n",
    "md.append(\"|---|---|---:|---|---:|---|---:|---|---:|---|---:|\")\n",
    "for r in rows_nbr:\n",
    "    md.append(f\"| {r['service']} | {r['n1']} | {r['s1']} | {r['n2']} | {r['s2']} | {r['n3']} | {r['s3']} | {r['n4']} | {r['s4']} | {r['n5']} | {r['s5']} |\")\n",
    "md.append(\"\")\n",
    "\n",
    "# Task 2 clustering summary\n",
    "md.append(\"## Task 2 — Clustering Summary (Silhouette, cosine)\")\n",
    "if rows_clu:\n",
    "    md.append(\"| Representation | Method | k | Silhouette |\")\n",
    "    md.append(\"|---|---|---:|---:|\")\n",
    "    for r in rows_clu:\n",
    "        md.append(f\"| {r['Representation']} | {r['Method']} | {r['k']} | {r['Silhouette']} |\")\n",
    "else:\n",
    "    md.append(\"_No clustering summary file found._\")\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(md))\n",
    "\n",
    "print(\"Saved:\", out_md)\n",
    "# preview first ~40 lines\n",
    "with open(out_md, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in zip(range(40), f):\n",
    "        print(line.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713e53ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task1_top_incidents.csv\n",
      "Top 3 preview:\n",
      "{'time_bucket': '2025-05-12 02:00:00', 'consumer_id': 'ECOMLP', 'supplier_id': 'MCSCBT', 'probability': 1.0, 'predicted_anomaly': 1, 'weak_label': 1, 'req_count': 6.0, 'error_rate': 0.0, 'cost_sum': 3539.9500000000007, 'cost_mean': 589.9916666666668, 'data_sum': 70.8, 'data_mean': 11.799999999999999}\n",
      "{'time_bucket': '2025-03-30 01:00:00', 'consumer_id': 'CUSTPRT', 'supplier_id': 'MCSCBT', 'probability': 1.0, 'predicted_anomaly': 1, 'weak_label': 1, 'req_count': 6.0, 'error_rate': 0.0, 'cost_sum': 3465.25, 'cost_mean': 577.5416666666666, 'data_sum': 73.05000000000001, 'data_mean': 12.175000000000002}\n",
      "{'time_bucket': '2025-03-25 05:00:00', 'consumer_id': 'ECOMLP', 'supplier_id': 'MCSCBT', 'probability': 1.0, 'predicted_anomaly': 1, 'weak_label': 1, 'req_count': 5.0, 'error_rate': 0.0, 'cost_sum': 2921.6, 'cost_mean': 584.3199999999999, 'data_sum': 59.25, 'data_mean': 11.85}\n"
     ]
    }
   ],
   "source": [
    "# Task 1 — Top-20 incidents (ranked by proba * cost_sum)\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# Rebuild weak labels (same as earlier)\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "z_c = robust_z(cmean); z_d = robust_z(dmean)\n",
    "y = ((er > 0.10) | (z_c > 3.0) | (z_d > 3.0)).astype(int)\n",
    "\n",
    "# Time-based split (80/20)\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]).fit(Xtr, ytr)\n",
    "\n",
    "proba = model.predict_proba(Xva)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "bi = int(np.argmax(f1s))\n",
    "best_thr = thr[bi-1] if bi>0 and (bi-1)<len(thr) else 0.5\n",
    "pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "# Rank by probability * cost_sum (impact proxy)\n",
    "cost_sum = Xva[:,2]\n",
    "score = proba * (cost_sum + 1e-9)\n",
    "order = np.argsort(-score)\n",
    "\n",
    "top_k = 20\n",
    "rows = []\n",
    "for r in order[:top_k]:\n",
    "    cons, supp, bucket = keys[valid_idx[r]]\n",
    "    req_count, error_rate, cost_sum, cost_mean, data_sum, data_mean = Xva[r].tolist()\n",
    "    rows.append({\n",
    "        \"time_bucket\": bucket,\n",
    "        \"consumer_id\": cons,\n",
    "        \"supplier_id\": supp,\n",
    "        \"probability\": float(proba[r]),\n",
    "        \"predicted_anomaly\": int(pred[r]),\n",
    "        \"weak_label\": int(yva[r]),\n",
    "        \"req_count\": float(req_count),\n",
    "        \"error_rate\": float(error_rate),\n",
    "        \"cost_sum\": float(cost_sum),\n",
    "        \"cost_mean\": float(cost_mean),\n",
    "        \"data_sum\": float(data_sum),\n",
    "        \"data_mean\": float(data_mean)\n",
    "    })\n",
    "\n",
    "out_path = \"task1_top_incidents.csv\"\n",
    "with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader()\n",
    "    for rr in rows: w.writerow(rr)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Top 3 preview:\")\n",
    "for rr in rows[:3]:\n",
    "    print(rr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b9be0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task1_incident_summaries.md\n",
      "\n",
      "Preview:\n",
      "\n",
      "# Top Incidents — Brief Summaries\n",
      "\n",
      "## 2025-05-12 02:00:00 — ECOMLP → MCSCBT\n",
      "- **Anomaly score**: p=1.000 (confidence: high), predicted=1, weak_label=1\n",
      "- **Impact**: req=6, cost_sum=$3,539.95, cost/tx=589.99, data_sum=70.80, data/tx=11.80\n",
      "- **Signals**: cost/tx high (z=14.7), data/tx high (z=8.2)\n",
      "- **Next checks**: verify recent deployments/config for `MCSCBT`, inspect capacity/quotas, correlate with upstream `ECOMLP` traffic and provider billing.\n",
      "\n",
      "## 2025-03-30 01:00:00 — CUSTPRT → MCSCBT\n",
      "- **Anomaly score**: p=1.000 (confidence: high), predicted=1, weak_label=1\n",
      "- **Impact**: req=6, cost_sum=$3,465.25, cost/tx=577.54, data_sum=73.05, data/tx=12.18\n",
      "- **Signals**: cost/tx high (z=14.4), data/tx high (z=8.4)\n",
      "- **Next checks**: verify recent deployments/config for `MCSCBT`, inspect capacity/quotas, correlate with upstream `CUSTPRT` traffic and provider billing.\n",
      "\n",
      "## 2025-03-25 05:00:00 — ECOMLP → MCSCBT\n",
      "- **Anomaly score**: p=1.000 (confidence: high), predicted=1, weak_label=1\n",
      "- **Impact**: req=5, cost_sum=$2,921.60, cost/tx=584.32, data_sum=59.25, data/tx=11.85\n",
      "- **Signals**: cost/tx high (z=14.6), data/tx high (z=8.2)\n",
      "- **Next checks**: verify recent deployments/config for `MCSCBT`, inspect capacity/quotas, correlate with upstream `ECOMLP` traffic and provider billing.\n"
     ]
    }
   ],
   "source": [
    "# Build concise LLM-style summaries (template) for Top-20 incidents\n",
    "import csv, math\n",
    "import numpy as np\n",
    "\n",
    "assert 'X' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# Globals from X to compute robust baselines\n",
    "# Features: [req_count, error_rate, cost_sum, cost_mean, data_sum, data_mean]\n",
    "cmean_all = X[:,3]; dmean_all = X[:,5]; er_all = X[:,1]\n",
    "\n",
    "def robust_stats(x):\n",
    "    med = float(np.median(x))\n",
    "    mad = float(np.median(np.abs(x - med))) + 1e-9\n",
    "    return med, mad\n",
    "\n",
    "c_med, c_mad = robust_stats(cmean_all)\n",
    "d_med, d_mad = robust_stats(dmean_all)\n",
    "e_med, e_mad = robust_stats(er_all)\n",
    "\n",
    "def rz(val, med, mad):\n",
    "    return (abs(val - med) / (1.4826 * mad)) if mad > 0 else 0.0\n",
    "\n",
    "def conf_str(p):\n",
    "    return \"high\" if p >= 0.9 else (\"medium\" if p >= 0.7 else \"low\")\n",
    "\n",
    "rows = []\n",
    "with open(\"task1_top_incidents.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = list(csv.DictReader(f))\n",
    "\n",
    "lines = [\"# Top Incidents — Brief Summaries\\n\"]\n",
    "for r in rows:\n",
    "    tb   = r[\"time_bucket\"]\n",
    "    cons = r[\"consumer_id\"]\n",
    "    supp = r[\"supplier_id\"]\n",
    "    proba = float(r[\"probability\"])\n",
    "    pred  = int(r[\"predicted_anomaly\"])\n",
    "    lab   = int(r[\"weak_label\"])\n",
    "    req   = float(r[\"req_count\"])\n",
    "    er    = float(r[\"error_rate\"])\n",
    "    csum  = float(r[\"cost_sum\"])\n",
    "    cmean = float(r[\"cost_mean\"])\n",
    "    dsum  = float(r[\"data_sum\"])\n",
    "    dmean = float(r[\"data_mean\"])\n",
    "\n",
    "    z_c = rz(cmean, c_med, c_mad)\n",
    "    z_d = rz(dmean, d_med, d_mad)\n",
    "    z_e = rz(er,    e_med, e_mad)\n",
    "\n",
    "    reasons = []\n",
    "    if er > max(0.10, e_med + 2*1.4826*e_mad): reasons.append(f\"error rate spike ({er:.1%})\")\n",
    "    if z_c > 3: reasons.append(f\"cost/tx high (z={z_c:.1f})\")\n",
    "    if z_d > 3: reasons.append(f\"data/tx high (z={z_d:.1f})\")\n",
    "    if not reasons:\n",
    "        reasons.append(\"unusual pattern vs baseline\")\n",
    "\n",
    "    lines.append(f\"## {tb} — {cons} → {supp}\")\n",
    "    lines.append(f\"- **Anomaly score**: p={proba:.3f} (confidence: {conf_str(proba)}), predicted={pred}, weak_label={lab}\")\n",
    "    lines.append(f\"- **Impact**: req={req:.0f}, cost_sum=${csum:,.2f}, cost/tx={cmean:.2f}, data_sum={dsum:.2f}, data/tx={dmean:.2f}\")\n",
    "    lines.append(f\"- **Signals**: {', '.join(reasons)}\")\n",
    "    lines.append(f\"- **Next checks**: verify recent deployments/config for `{supp}`, inspect capacity/quotas, correlate with upstream `{cons}` traffic and provider billing.\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "out_md = \"task1_incident_summaries.md\"\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Saved:\", out_md)\n",
    "print(\"\\nPreview:\\n\")\n",
    "print(\"\\n\".join(lines[:18]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8caec9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: task1_results_variants.csv\n",
      "{'Feature Set / Variant': 'Counts only (NB)', 'Precision': 0.10568402532383772, 'Recall': 0.8179001721170396, 'F1': 0.1830056388612474, 'PR-AUC': 0.1021308496893949, 'Accuracy (+ve)': 0.9179575444635686, 'Accuracy (mean)': 0.3043420390540036, 'Threshold': 0.3514217132994965, 'Valid size': 308035, 'Pos rate (valid)': 0.0848767185547097}\n",
      "{'Feature Set / Variant': 'All features (NB)', 'Precision': 0.9028533849694933, 'Recall': 0.9451902849493211, 'F1': 0.9235196292766784, 'PR-AUC': 0.9774733165108653, 'Accuracy (+ve)': 0.9451902849493211, 'Accuracy (mean)': 0.9867125488986641, 'Threshold': 0.9806067807015026, 'Valid size': 308035, 'Pos rate (valid)': 0.0848767185547097}\n",
      "{'Feature Set / Variant': 'PCA(3) + NB', 'Precision': 0.6810904129423886, 'Recall': 0.9355517307324537, 'F1': 0.7882821186290465, 'PR-AUC': 0.6950759672884285, 'Accuracy (+ve)': 0.9355517307324537, 'Accuracy (mean)': 0.9573457561640722, 'Threshold': 0.154903618666408, 'Valid size': 308035, 'Pos rate (valid)': 0.0848767185547097}\n"
     ]
    }
   ],
   "source": [
    "# Task 1 variants: Counts-only (NB), All features (NB), PCA(3)+NB)\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, f1_score, accuracy_score\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# --- weak labels (same as before) ---\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "z_c = robust_z(cmean); z_d = robust_z(dmean)\n",
    "y = ((er > 0.10) | (z_c > 3.0) | (z_d > 3.0)).astype(int)\n",
    "\n",
    "# time-based split\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "Xtr_all, ytr = X[train_idx], y[train_idx]\n",
    "Xva_all, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "def eval_model(model, Xtr, ytr, Xva, yva, label):\n",
    "    model.fit(Xtr, ytr)\n",
    "    proba = model.predict_proba(Xva)[:,1]\n",
    "    pr_auc = average_precision_score(yva, proba) if len(np.unique(yva))>1 else 0.0\n",
    "    prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "    f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "    bi = int(np.argmax(f1s))\n",
    "    best_thr = thr[bi-1] if bi>0 and (bi-1)<len(thr) else 0.5\n",
    "    yhat = (proba >= best_thr).astype(int)\n",
    "    acc_pos = accuracy_score(yva[yva==1], yhat[yva==1]) if np.any(yva==1) else 0.0\n",
    "    acc_mean = accuracy_score(yva, yhat)\n",
    "    return {\n",
    "        \"Feature Set / Variant\": label,\n",
    "        \"Precision\": float(prec[bi]),\n",
    "        \"Recall\": float(rec[bi]),\n",
    "        \"F1\": float(f1_score(yva, yhat)),\n",
    "        \"PR-AUC\": float(pr_auc),\n",
    "        \"Accuracy (+ve)\": float(acc_pos),\n",
    "        \"Accuracy (mean)\": float(acc_mean),\n",
    "        \"Threshold\": float(best_thr),\n",
    "        \"Valid size\": int(len(yva)),\n",
    "        \"Pos rate (valid)\": float(yva.mean())\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Variant 1: Counts only (req_count, error_rate) + NB\n",
    "rows.append(\n",
    "    eval_model(\n",
    "        Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]),\n",
    "        Xtr_all[:, [0,1]], ytr,\n",
    "        Xva_all[:, [0,1]], yva,\n",
    "        \"Counts only (NB)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Variant 2: All features (NB)\n",
    "rows.append(\n",
    "    eval_model(\n",
    "        Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]),\n",
    "        Xtr_all, ytr, Xva_all, yva,\n",
    "        \"All features (NB)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Variant 3: PCA(3) + NB\n",
    "rows.append(\n",
    "    eval_model(\n",
    "        Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=3, random_state=42)), (\"nb\", GaussianNB())]),\n",
    "        Xtr_all, ytr, Xva_all, yva,\n",
    "        \"PCA(3) + NB\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save multi-row CSV\n",
    "out_csv = \"task1_results_variants.csv\"\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader()\n",
    "    for r in rows: w.writerow(r)\n",
    "\n",
    "print(\"Saved:\", out_csv)\n",
    "for r in rows: print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a77a9935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tables_for_report_full.md\n"
     ]
    }
   ],
   "source": [
    "# Build FINAL report tables (includes Task 1 variants + Task 2)\n",
    "import csv, os\n",
    "\n",
    "t1_var_csv = \"task1_results_variants.csv\"\n",
    "t2_nbr_csv = \"task2_neighbors.csv\"\n",
    "t2_clu_csv = \"task2_clustering_results.csv\"\n",
    "out_md = \"tables_for_report_full.md\"\n",
    "\n",
    "# --- load Task 1 variants ---\n",
    "with open(t1_var_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "    t1_rows = list(csv.DictReader(f))\n",
    "\n",
    "# --- load Task 2 neighbors (sample 10) ---\n",
    "rows_nbr = []\n",
    "if os.path.exists(t2_nbr_csv):\n",
    "    with open(t2_nbr_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows_nbr = list(csv.DictReader(f))[:10]\n",
    "\n",
    "# --- load Task 2 clustering summary ---\n",
    "rows_clu = []\n",
    "if os.path.exists(t2_clu_csv):\n",
    "    with open(t2_clu_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows_clu = list(csv.DictReader(f))\n",
    "\n",
    "lines = []\n",
    "lines.append(\"# Results Tables (Final)\\n\")\n",
    "\n",
    "# ---- Task 1 variants table ----\n",
    "hdr = [\"Feature Set / Variant\",\"Precision\",\"Recall\",\"F1\",\"PR-AUC\",\"Accuracy (+ve)\",\"Accuracy (mean)\",\"Threshold\",\"Valid size\",\"Pos rate (valid)\"]\n",
    "lines.append(\"## Task 1 — Insight Detection (Naive Bayes Variants)\\n\")\n",
    "lines.append(\"| \" + \" | \".join(hdr) + \" |\")\n",
    "lines.append(\"|\" + \"|\".join([\"---\"] + [\":---:\" for _ in hdr[1:]]) + \"|\")\n",
    "for r in t1_rows:\n",
    "    lines.append(\"| \" + \" | \".join(str(r[h]) for h in hdr) + \" |\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# ---- Task 2 neighbors (sample) ----\n",
    "lines.append(\"## Task 2 — Service Similarity (Top-5 Neighbors, sample)\")\n",
    "nbr_hdr = [\"service\",\"n1\",\"s1\",\"n2\",\"s2\",\"n3\",\"s3\",\"n4\",\"s4\",\"n5\",\"s5\"]\n",
    "lines.append(\"| \" + \" | \".join(nbr_hdr) + \" |\")\n",
    "lines.append(\"|\" + \"|\".join([\"---\",\"---\",\":---:\",\"---\",\":---:\",\"---\",\":---:\",\"---\",\":---:\",\"---\",\":---:\"]) + \"|\")\n",
    "for r in rows_nbr:\n",
    "    lines.append(\"| \" + \" | \".join(r[h] for h in nbr_hdr) + \" |\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# ---- Task 2 clustering summary ----\n",
    "lines.append(\"## Task 2 — Clustering Summary (Silhouette, cosine)\")\n",
    "if rows_clu:\n",
    "    lines.append(\"| Representation | Method | k | Silhouette |\")\n",
    "    lines.append(\"|---|---|---:|---:|\")\n",
    "    for r in rows_clu:\n",
    "        lines.append(f\"| {r['Representation']} | {r['Method']} | {r['k']} | {r['Silhouette']} |\")\n",
    "else:\n",
    "    lines.append(\"_No clustering summary file found._\")\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Saved:\", out_md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a54699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEYS ===\n",
      "['app/id', 'app/name', 'daily/label', 'daily/metric', 'daily/time', 'daily/value', 'tenant/id']\n",
      "\n",
      "=== FIRST 5 ROWS ===\n",
      "\n",
      "--- Row 1 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'daily/label': 'Cost', 'daily/metric': 'cost', 'daily/time': '2024-06-01T00:00:00', 'daily/value': 158758.2500000005, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 2 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'daily/label': 'Value', 'daily/metric': 'value', 'daily/time': '2024-06-01T00:00:00', 'daily/value': 4646.0999999999985, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 3 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'daily/label': 'Data Used', 'daily/metric': 'data_used', 'daily/time': '2024-06-01T00:00:00', 'daily/value': 4646.100000000008, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 4 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'daily/label': 'Data Sent', 'daily/metric': 'data_sent', 'daily/time': '2024-06-01T00:00:00', 'daily/value': 4646.0999999999985, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 5 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'daily/label': 'Requests Made', 'daily/metric': 'requests_made', 'daily/time': '2024-06-01T00:00:00', 'daily/value': 5490, 'tenant/id': 'DEMO'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect daily_metrics.jsonl (schema + first rows)\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "def inspect_jsonl(path: str, n: int = 5):\n",
    "    keys_union, samples = set(), []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in islice(f, n):\n",
    "            if not line.strip(): \n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            samples.append(obj)\n",
    "            keys_union.update(obj.keys())\n",
    "    keys = sorted(keys_union)\n",
    "    print(\"=== KEYS ===\")\n",
    "    print(keys)\n",
    "    print(f\"\\n=== FIRST {len(samples)} ROWS ===\")\n",
    "    for i, obj in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Row {i} ---\")\n",
    "        print({k: obj.get(k) for k in keys})\n",
    "\n",
    "inspect_jsonl(\"daily_metrics.jsonl\", n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "740b3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read: 444570\n",
      "Apps: 87\n",
      "Days (unique): 365\n",
      "Metrics included: ['cost', 'value', 'data_used', 'data_sent', 'requests_made', 'cost_per_request_made', 'cost_per_request_received', 'data_per_request', 'data_sent_per_received', 'data_used_per_received', 'requests_per_business_hour', 'requests_per_hour', 'requests_received', 'value_per_cost']\n",
      "Saved: daily_features.csv\n",
      "\n",
      "Preview:\n",
      "{'app_id': 'SELENE', 'day': '2024-06-01', 'cost': 158758.2500000005, 'value': 4646.0999999999985, 'data_used': 4646.100000000008, 'data_sent': 4646.0999999999985, 'requests_made': 5490.0, 'cost_per_request_made': 28.917714025501002, 'cost_per_request_received': 86.753142076503, 'data_per_request': 0.8462841530054659, 'data_sent_per_received': 2.538852459016393, 'data_used_per_received': 2.5388524590163977, 'requests_per_business_hour': 296.8, 'requests_per_hour': 228.75, 'requests_received': 1830.0, 'value_per_cost': 0.02926525078224271, 'cost_per_request': 28.917714025501002}\n",
      "{'app_id': 'AWS', 'day': '2024-06-01', 'cost': 0.0, 'value': 1342.6499999999983, 'data_used': 0.0, 'data_sent': 1342.6499999999983, 'requests_made': 0.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 0.2919439008480101, 'data_used_per_received': 0.0, 'requests_per_business_hour': 185.5, 'requests_per_hour': 0.0, 'requests_received': 4599.0, 'value_per_cost': 1342649999999.9983, 'cost_per_request': 0.0}\n",
      "{'app_id': 'DBRCKS', 'day': '2024-06-01', 'cost': 0.0, 'value': 3673.200000000003, 'data_used': 0.0, 'data_sent': 3673.200000000003, 'requests_made': 0.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 1.0727803738317765, 'data_used_per_received': 0.0, 'requests_per_business_hour': 138.5, 'requests_per_hour': 0.0, 'requests_received': 3424.0, 'value_per_cost': 3673200000000.003, 'cost_per_request': 0.0}\n",
      "{'app_id': 'SNWFLK', 'day': '2024-06-01', 'cost': 0.0, 'value': 2185.6499999999983, 'data_used': 0.0, 'data_sent': 2185.6499999999983, 'requests_made': 0.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 0.9675298804780869, 'data_used_per_received': 0.0, 'requests_per_business_hour': 90.6, 'requests_per_hour': 0.0, 'requests_received': 2259.0, 'value_per_cost': 2185649999999.998, 'cost_per_request': 0.0}\n",
      "{'app_id': 'FRDETCT', 'day': '2024-06-01', 'cost': 11736.2, 'value': 176.24999999999997, 'data_used': 268.05, 'data_sent': 176.24999999999997, 'requests_made': 192.0, 'cost_per_request_made': 61.12604166666667, 'cost_per_request_received': 286.2487804878049, 'data_per_request': 1.3960937500000001, 'data_sent_per_received': 4.298780487804877, 'data_used_per_received': 6.537804878048781, 'requests_per_business_hour': 11.6, 'requests_per_hour': 8.0, 'requests_received': 41.0, 'value_per_cost': 0.015017637736234895, 'cost_per_request': 61.12604166666667}\n"
     ]
    }
   ],
   "source": [
    "# Pivot daily_metrics.jsonl -> daily_features.csv (per-app per-day)\n",
    "import json, csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "path = \"daily_metrics.jsonl\"\n",
    "\n",
    "def to_day(s: str):\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d %H:%M:%S\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Accumulate values per (app_id, day, metric)\n",
    "agg = defaultdict(lambda: defaultdict(float))\n",
    "apps = set()\n",
    "metrics_seen = set()\n",
    "rows_read = 0\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        app = obj.get(\"app/id\")\n",
    "        day = to_day(obj.get(\"daily/time\", \"\"))\n",
    "        metric = obj.get(\"daily/metric\")\n",
    "        val = obj.get(\"daily/value\", 0.0) or 0.0\n",
    "        if not app or not day or not metric:\n",
    "            continue\n",
    "        agg[(app, day)][metric] += float(val)\n",
    "        apps.add(app)\n",
    "        metrics_seen.add(metric)\n",
    "        rows_read += 1\n",
    "\n",
    "metrics_base = [\"cost\", \"value\", \"data_used\", \"data_sent\", \"requests_made\"]\n",
    "ordered_metrics = [m for m in metrics_base if m in metrics_seen] + \\\n",
    "                  sorted([m for m in metrics_seen if m not in metrics_base])\n",
    "\n",
    "# Build rows + simple derived features\n",
    "out_rows = []\n",
    "for (app, day), mvals in agg.items():\n",
    "    row = {\"app_id\": app, \"day\": day}\n",
    "    for m in ordered_metrics:\n",
    "        row[m] = mvals.get(m, 0.0)\n",
    "\n",
    "    req  = row.get(\"requests_made\", 0.0)\n",
    "    cost = row.get(\"cost\", 0.0)\n",
    "    val  = row.get(\"value\", 0.0)\n",
    "    data_u = row.get(\"data_used\", 0.0)\n",
    "\n",
    "    row[\"cost_per_request\"] = (cost / max(req, 1.0)) if \"cost\" in ordered_metrics else 0.0\n",
    "    row[\"value_per_cost\"]   = (val / max(cost, 1e-9)) if (\"value\" in ordered_metrics and \"cost\" in ordered_metrics) else 0.0\n",
    "    row[\"data_per_request\"] = (data_u / max(req, 1.0)) if \"data_used\" in ordered_metrics else 0.0\n",
    "\n",
    "    out_rows.append(row)\n",
    "\n",
    "fieldnames = [\"app_id\", \"day\"] + ordered_metrics + [\"cost_per_request\", \"value_per_cost\", \"data_per_request\"]\n",
    "csv_path = \"daily_features.csv\"\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in out_rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(\"Rows read:\", rows_read)\n",
    "print(\"Apps:\", len(apps))\n",
    "print(\"Days (unique):\", len({d for _, d in agg.keys()}))\n",
    "print(\"Metrics included:\", ordered_metrics)\n",
    "print(\"Saved:\", csv_path)\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "for r in out_rows[:5]:\n",
    "    print({k: r.get(k) for k in fieldnames})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb8485e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KEYS ===\n",
      "['app/id', 'app/name', 'monthly/created', 'monthly/label', 'monthly/metric', 'monthly/value', 'tenant/id']\n",
      "\n",
      "=== FIRST 5 ROWS ===\n",
      "\n",
      "--- Row 1 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'monthly/created': '2024-08-01T00:00:00', 'monthly/label': 'Cost', 'monthly/metric': 'cost', 'monthly/value': 1219070.7999999553, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 2 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'monthly/created': '2024-08-01T00:00:00', 'monthly/label': 'Value', 'monthly/metric': 'value', 'monthly/value': 35701.04999999981, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 3 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'monthly/created': '2024-08-01T00:00:00', 'monthly/label': 'Data Used', 'monthly/metric': 'data_used', 'monthly/value': 35701.049999999675, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 4 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'monthly/created': '2024-08-01T00:00:00', 'monthly/label': 'Data Sent', 'monthly/metric': 'data_sent', 'monthly/value': 35701.04999999981, 'tenant/id': 'DEMO'}\n",
      "\n",
      "--- Row 5 ---\n",
      "{'app/id': 'SELENE', 'app/name': 'Selene Customer Warehouse', 'monthly/created': '2024-08-01T00:00:00', 'monthly/label': 'Requests Made', 'monthly/metric': 'requests_made', 'monthly/value': 41964, 'tenant/id': 'DEMO'}\n"
     ]
    }
   ],
   "source": [
    "# Inspect monthly_metrics.jsonl (schema + first rows)\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "def inspect_jsonl(path: str, n: int = 5):\n",
    "    keys_union, samples = set(), []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in islice(f, n):\n",
    "            if not line.strip(): \n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            samples.append(obj)\n",
    "            keys_union.update(obj.keys())\n",
    "    keys = sorted(keys_union)\n",
    "    print(\"=== KEYS ===\")\n",
    "    print(keys)\n",
    "    print(f\"\\n=== FIRST {len(samples)} ROWS ===\")\n",
    "    for i, obj in enumerate(samples, 1):\n",
    "        print(f\"\\n--- Row {i} ---\")\n",
    "        print({k: obj.get(k) for k in keys})\n",
    "\n",
    "inspect_jsonl(\"monthly_metrics.jsonl\", n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3c9fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows read: 18270\n",
      "Apps: 87\n",
      "Months (unique): 10\n",
      "Metrics included: ['cost', 'value', 'data_used', 'data_sent', 'requests_made', 'requests_received', 'cost_per_request_made', 'cost_per_request_received', 'data_per_request', 'data_sent_per_received', 'data_used_per_received', 'outage_cost', 'outage_count', 'outage_duration', 'outage_efficiency', 'outage_frequency', 'outage_impact', 'outage_severity', 'rate_of_return', 'requests_per_business_hour', 'requests_per_hour']\n",
      "Saved: monthly_features.csv\n",
      "\n",
      "Preview:\n",
      "{'app_id': 'SELENE', 'month': '2024-08', 'cost': 1219070.7999999553, 'value': 35701.04999999981, 'data_used': 35701.049999999675, 'data_sent': 35701.04999999981, 'requests_made': 41964.0, 'requests_received': 13988.0, 'cost_per_request_made': 29.050395577160312, 'cost_per_request_received': 87.15118673148093, 'data_per_request': 0.8507542179010503, 'data_sent_per_received': 2.5522626537031603, 'data_used_per_received': 2.552262653703151, 'outage_cost': 0.0, 'outage_count': 2.0, 'outage_duration': 205.0, 'outage_efficiency': 0.0, 'outage_frequency': 1.0, 'outage_impact': 0.0, 'outage_severity': 102.5, 'rate_of_return': 0.029285460696787353, 'requests_per_business_hour': 264.6666666666667, 'requests_per_hour': 194.27777777777777, 'cost_per_request': 29.050395577160312, 'value_per_cost': 0.029285460696787353}\n",
      "{'app_id': 'AWS', 'month': '2024-08', 'cost': 0.0, 'value': 10423.349999999726, 'data_used': 0.0, 'data_sent': 10423.349999999726, 'requests_made': 0.0, 'requests_received': 35749.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 0.291570393577435, 'data_used_per_received': 0.0, 'outage_cost': 0.0, 'outage_count': 0.0, 'outage_duration': 0.0, 'outage_efficiency': 0.0, 'outage_frequency': 0.0, 'outage_impact': 0.0, 'outage_severity': 0.0, 'rate_of_return': 0.0, 'requests_per_business_hour': 167.36666666666667, 'requests_per_hour': 0.0, 'cost_per_request': 0.0, 'value_per_cost': 10423349999999.725}\n",
      "{'app_id': 'DBRCKS', 'month': '2024-08', 'cost': 0.0, 'value': 27726.45000000023, 'data_used': 0.0, 'data_sent': 27726.45000000023, 'requests_made': 0.0, 'requests_received': 25793.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 1.0749602605358133, 'data_used_per_received': 0.0, 'outage_cost': 0.0, 'outage_count': 0.0, 'outage_duration': 0.0, 'outage_efficiency': 0.0, 'outage_frequency': 0.0, 'outage_impact': 0.0, 'outage_severity': 0.0, 'rate_of_return': 0.0, 'requests_per_business_hour': 121.88888888888889, 'requests_per_hour': 0.0, 'cost_per_request': 0.0, 'value_per_cost': 27726450000000.227}\n",
      "{'app_id': 'SNWFLK', 'month': '2024-08', 'cost': 0.0, 'value': 17358.6, 'data_used': 0.0, 'data_sent': 17358.6, 'requests_made': 0.0, 'requests_received': 17780.0, 'cost_per_request_made': 0.0, 'cost_per_request_received': 0.0, 'data_per_request': 0.0, 'data_sent_per_received': 0.9762992125984251, 'data_used_per_received': 0.0, 'outage_cost': 0.0, 'outage_count': 0.0, 'outage_duration': 0.0, 'outage_efficiency': 0.0, 'outage_frequency': 0.0, 'outage_impact': 0.0, 'outage_severity': 0.0, 'rate_of_return': 0.0, 'requests_per_business_hour': 84.47777777777777, 'requests_per_hour': 0.0, 'cost_per_request': 0.0, 'value_per_cost': 17358599999999.998}\n",
      "{'app_id': 'RISKMG', 'month': '2024-08', 'cost': 67964.55000000002, 'value': 0.0, 'data_used': 1541.5499999999988, 'data_sent': 0.0, 'requests_made': 606.0, 'requests_received': 0.0, 'cost_per_request_made': 112.15272277227726, 'cost_per_request_received': 0.0, 'data_per_request': 2.543811881188117, 'data_sent_per_received': 0.0, 'data_used_per_received': 0.0, 'outage_cost': 0.0, 'outage_count': 0.0, 'outage_duration': 0.0, 'outage_efficiency': 0.0, 'outage_frequency': 0.0, 'outage_impact': 0.0, 'outage_severity': 0.0, 'rate_of_return': 0.0, 'requests_per_business_hour': 2.7, 'requests_per_hour': 2.8055555555555554, 'cost_per_request': 112.15272277227726, 'value_per_cost': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Pivot monthly_metrics.jsonl -> monthly_features.csv (per-app per-month)\n",
    "import json, csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "path = \"monthly_metrics.jsonl\"\n",
    "\n",
    "def to_month(s: str):\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d %H:%M:%S\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt).strftime(\"%Y-%m\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Accumulate values per (app_id, month, metric)\n",
    "agg = defaultdict(lambda: defaultdict(float))\n",
    "apps = set()\n",
    "metrics_seen = set()\n",
    "rows_read = 0\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        app = obj.get(\"app/id\")\n",
    "        month = to_month(obj.get(\"monthly/created\", \"\"))  # timestamp field\n",
    "        metric = obj.get(\"monthly/metric\")\n",
    "        val = obj.get(\"monthly/value\", 0.0) or 0.0\n",
    "        if not app or not month or not metric:\n",
    "            continue\n",
    "        agg[(app, month)][metric] += float(val)\n",
    "        apps.add(app)\n",
    "        metrics_seen.add(metric)\n",
    "        rows_read += 1\n",
    "\n",
    "metrics_base = [\"cost\",\"value\",\"data_used\",\"data_sent\",\"requests_made\",\"requests_received\"]\n",
    "ordered_metrics = [m for m in metrics_base if m in metrics_seen] + \\\n",
    "                  sorted([m for m in metrics_seen if m not in metrics_base])\n",
    "\n",
    "# Build rows + simple derived features\n",
    "out_rows = []\n",
    "for (app, month), mvals in agg.items():\n",
    "    row = {\"app_id\": app, \"month\": month}\n",
    "    for m in ordered_metrics:\n",
    "        row[m] = mvals.get(m, 0.0)\n",
    "\n",
    "    req  = row.get(\"requests_made\", 0.0)\n",
    "    cost = row.get(\"cost\", 0.0)\n",
    "    val  = row.get(\"value\", 0.0)\n",
    "    data_u = row.get(\"data_used\", 0.0)\n",
    "\n",
    "    row[\"cost_per_request\"] = (cost / max(req, 1.0)) if \"cost\" in ordered_metrics else 0.0\n",
    "    row[\"value_per_cost\"]   = (val / max(cost, 1e-9)) if (\"value\" in ordered_metrics and \"cost\" in ordered_metrics) else 0.0\n",
    "    row[\"data_per_request\"] = (data_u / max(req, 1.0)) if \"data_used\" in ordered_metrics else 0.0\n",
    "\n",
    "    out_rows.append(row)\n",
    "\n",
    "fieldnames = [\"app_id\",\"month\"] + ordered_metrics + [\"cost_per_request\",\"value_per_cost\",\"data_per_request\"]\n",
    "csv_path = \"monthly_features.csv\"\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    w.writeheader()\n",
    "    for r in out_rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(\"Rows read:\", rows_read)\n",
    "print(\"Apps:\", len(apps))\n",
    "print(\"Months (unique):\", len({m for _, m in agg.keys()}))\n",
    "print(\"Metrics included:\", ordered_metrics)\n",
    "print(\"Saved:\", csv_path)\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "for r in out_rows[:5]:\n",
    "    print({k: r.get(k) for k in fieldnames})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: watchlist_apps.csv\n",
      "Top 10 preview:\n",
      "{'app_id': 'MCSCBT', 'anomalies_consumer': 3010, 'score_consumer': 2247204.864172169, 'anomalies_supplier': 3069, 'score_supplier': 2627282.000003058, 'anomalies_total': 6079, 'score_total': 4874486.864175227, 'latest_month': '2025-05', 'cost_latest': 1522083.049999968, 'cost_mom_pct': 0.07695582850548877, 'value_per_cost_latest': 0.015829359639738805, 'outage_count_latest': 83.0, 'outage_duration_latest': 13955.0}\n",
      "{'app_id': 'CLVMDL', 'anomalies_consumer': 0, 'score_consumer': 4555.476827580488, 'anomalies_supplier': 6695, 'score_supplier': 2512970.4724194533, 'anomalies_total': 6695, 'score_total': 2517525.949247034, 'latest_month': '2025-05', 'cost_latest': 1249448.799999955, 'cost_mom_pct': 0.06576045593726959, 'value_per_cost_latest': 0.01809545937376611, 'outage_count_latest': 287.0, 'outage_duration_latest': 85770.0}\n",
      "{'app_id': 'CSTSEG', 'anomalies_consumer': 0, 'score_consumer': 10762.728691977702, 'anomalies_supplier': 3332, 'score_supplier': 1569913.3446396294, 'anomalies_total': 3332, 'score_total': 1580676.073331607, 'latest_month': '2025-05', 'cost_latest': 849081.7000000023, 'cost_mom_pct': 0.06634769737528308, 'value_per_cost_latest': 0.014266824971024585, 'outage_count_latest': 1571.0, 'outage_duration_latest': 239785.0}\n",
      "{'app_id': 'SELENE', 'anomalies_consumer': 0, 'score_consumer': 2.0304696855699883e-134, 'anomalies_supplier': 0, 'score_supplier': 1402166.6285491423, 'anomalies_total': 0, 'score_total': 1402166.6285491423, 'latest_month': '2025-05', 'cost_latest': 4452273.550000766, 'cost_mom_pct': 0.04600896213907379, 'value_per_cost_latest': 0.029246720476098772, 'outage_count_latest': 2.0, 'outage_duration_latest': 205.0}\n",
      "{'app_id': 'CUSTPRT', 'anomalies_consumer': 2398, 'score_consumer': 1370124.0668129628, 'anomalies_supplier': 0, 'score_supplier': 0.0, 'anomalies_total': 2398, 'score_total': 1370124.0668129628, 'latest_month': '2025-05', 'cost_latest': 708209.9499999983, 'cost_mom_pct': 0.10269449470147067, 'value_per_cost_latest': 0.0, 'outage_count_latest': 16.0, 'outage_duration_latest': 1505.0}\n",
      "{'app_id': 'MOBAPP', 'anomalies_consumer': 2332, 'score_consumer': 1359245.272597053, 'anomalies_supplier': 0, 'score_supplier': 0.0, 'anomalies_total': 2332, 'score_total': 1359245.272597053, 'latest_month': '2025-05', 'cost_latest': 839258.6499999962, 'cost_mom_pct': 0.1474099290780082, 'value_per_cost_latest': 0.0, 'outage_count_latest': 94.0, 'outage_duration_latest': 7320.0}\n",
      "{'app_id': 'NXPCR', 'anomalies_consumer': 0, 'score_consumer': 39375.629084546905, 'anomalies_supplier': 1662, 'score_supplier': 1264470.6331996147, 'anomalies_total': 1662, 'score_total': 1303846.2622841615, 'latest_month': '2025-05', 'cost_latest': 709015.0499999976, 'cost_mom_pct': 0.09823546427537842, 'value_per_cost_latest': 0.017724800058898628, 'outage_count_latest': 68.0, 'outage_duration_latest': 9765.0}\n",
      "{'app_id': 'ECOMLP', 'anomalies_consumer': 1525, 'score_consumer': 1188310.9207149453, 'anomalies_supplier': 0, 'score_supplier': 0.0, 'anomalies_total': 1525, 'score_total': 1188310.9207149453, 'latest_month': '2025-05', 'cost_latest': 760425.2499999962, 'cost_mom_pct': 0.0675479634819133, 'value_per_cost_latest': 0.0, 'outage_count_latest': 3.0, 'outage_duration_latest': 395.0}\n",
      "{'app_id': 'MLOPS', 'anomalies_consumer': 2792, 'score_consumer': 1061194.7956829409, 'anomalies_supplier': 0, 'score_supplier': 0.0, 'anomalies_total': 2792, 'score_total': 1061194.7956829409, 'latest_month': '2025-05', 'cost_latest': 456126.49999999924, 'cost_mom_pct': 0.08050451725798735, 'value_per_cost_latest': 0.0, 'outage_count_latest': 0.0, 'outage_duration_latest': 0.0}\n",
      "{'app_id': 'FEATSTR', 'anomalies_consumer': 0, 'score_consumer': 54346.14585472198, 'anomalies_supplier': 3021, 'score_supplier': 889759.6047474698, 'anomalies_total': 3021, 'score_total': 944105.7506021918, 'latest_month': '2025-05', 'cost_latest': 374757.45000000106, 'cost_mom_pct': 0.028273741744478362, 'value_per_cost_latest': 0.01912917274893397, 'outage_count_latest': 0.0, 'outage_duration_latest': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Build a ranked WATCHLIST of apps (combine Task 1 anomalies + monthly trends)\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the transactions feature cell first.\"\n",
    "\n",
    "# ---------- Re-train Task 1 model on same split ----------\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "y = ((er > 0.10) | (robust_z(cmean) > 3.0) | (robust_z(dmean) > 3.0)).astype(int)\n",
    "\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]).fit(Xtr, ytr)\n",
    "proba = model.predict_proba(Xva)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "bi = int(np.argmax(f1s))\n",
    "best_thr = thr[bi-1] if bi>0 and (bi-1) < len(thr) else 0.5\n",
    "pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "# ---------- Aggregate anomalies per app (consumer & supplier roles) ----------\n",
    "cons_stats = defaultdict(lambda: {\"count\":0, \"score\":0.0})\n",
    "supp_stats = defaultdict(lambda: {\"count\":0, \"score\":0.0})\n",
    "\n",
    "for va_idx, glob_idx in enumerate(valid_idx):\n",
    "    cons, supp, _bucket = keys[glob_idx]\n",
    "    req_count, error_rate, cost_sum, cost_mean, data_sum, data_mean = Xva[va_idx]\n",
    "    p = proba[va_idx]; yhat = pred[va_idx]\n",
    "    impact = p * (cost_sum + 1e-9)\n",
    "    if yhat == 1:\n",
    "        cons_stats[cons][\"count\"] += 1\n",
    "        supp_stats[supp][\"count\"] += 1\n",
    "    cons_stats[cons][\"score\"] += impact\n",
    "    supp_stats[supp][\"score\"] += impact\n",
    "\n",
    "# ---------- Monthly trends (latest + MoM change) ----------\n",
    "monthly_path = \"monthly_features.csv\"\n",
    "monthly = defaultdict(dict)  # monthly[app][month] = metrics dict\n",
    "try:\n",
    "    with open(monthly_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            app = row[\"app_id\"]; month = row[\"month\"]\n",
    "            monthly[app][month] = row\n",
    "except FileNotFoundError:\n",
    "    pass  # if monthly not present, we will just skip trend columns\n",
    "\n",
    "def latest_mom(app):\n",
    "    if app not in monthly or not monthly[app]:\n",
    "        return (\"\", \"\", \"\", \"\", \"\")\n",
    "    months = sorted(monthly[app].keys())  # 'YYYY-MM' sorts lexicographically\n",
    "    latest = months[-1]\n",
    "    prev = months[-2] if len(months) >= 2 else None\n",
    "    r_latest = monthly[app][latest]\n",
    "    def fget(k):\n",
    "        try: return float(r_latest.get(k, \"0\") or 0.0)\n",
    "        except: return 0.0\n",
    "    cost_latest = fget(\"cost\")\n",
    "    vpc_latest = fget(\"value_per_cost\")\n",
    "    oc_latest  = float(r_latest.get(\"outage_count\", 0.0) or 0.0)\n",
    "    od_latest  = float(r_latest.get(\"outage_duration\", 0.0) or 0.0)\n",
    "    if prev:\n",
    "        try:\n",
    "            cost_prev = float((monthly[app][prev].get(\"cost\", \"0\") or 0.0))\n",
    "            mom = (cost_latest - cost_prev) / (cost_prev + 1e-9)\n",
    "        except:\n",
    "            mom = \"\"\n",
    "    else:\n",
    "        mom = \"\"\n",
    "    return (latest, cost_latest, mom, vpc_latest, (oc_latest, od_latest))\n",
    "\n",
    "# ---------- Build watchlist rows ----------\n",
    "apps = set(list(cons_stats.keys()) + list(supp_stats.keys()) + list(monthly.keys()))\n",
    "rows = []\n",
    "for app in apps:\n",
    "    c = cons_stats.get(app, {\"count\":0,\"score\":0.0})\n",
    "    s = supp_stats.get(app, {\"count\":0,\"score\":0.0})\n",
    "    latest, cost_latest, mom, vpc, outages = latest_mom(app)\n",
    "    oc, od = (outages if outages else (0.0, 0.0))\n",
    "    rows.append({\n",
    "        \"app_id\": app,\n",
    "        \"anomalies_consumer\": int(c[\"count\"]),\n",
    "        \"score_consumer\": float(c[\"score\"]),\n",
    "        \"anomalies_supplier\": int(s[\"count\"]),\n",
    "        \"score_supplier\": float(s[\"score\"]),\n",
    "        \"anomalies_total\": int(c[\"count\"] + s[\"count\"]),\n",
    "        \"score_total\": float(c[\"score\"] + s[\"score\"]),\n",
    "        \"latest_month\": latest,\n",
    "        \"cost_latest\": float(cost_latest) if cost_latest != \"\" else \"\",\n",
    "        \"cost_mom_pct\": float(mom) if mom != \"\" else \"\",\n",
    "        \"value_per_cost_latest\": float(vpc) if vpc != \"\" else \"\",\n",
    "        \"outage_count_latest\": float(oc),\n",
    "        \"outage_duration_latest\": float(od)\n",
    "    })\n",
    "\n",
    "# Rank by score_total desc\n",
    "rows.sort(key=lambda r: r[\"score_total\"], reverse=True)\n",
    "\n",
    "# Save CSV + preview\n",
    "out_csv = \"watchlist_apps.csv\"\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    fn = list(rows[0].keys()) if rows else []\n",
    "    w = csv.DictWriter(f, fieldnames=fn)\n",
    "    w.writeheader()\n",
    "    for r in rows: w.writerow(r)\n",
    "\n",
    "print(\"Saved:\", out_csv)\n",
    "print(\"Top 10 preview:\")\n",
    "for r in rows[:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eff56c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: report_pack.md\n",
      "# Trufflow 1B — Data Product Insight Recommendation\n",
      "\n",
      "## Summary\n",
      "- **Task 1 (Insight Detection, NB)**: strong PR-AUC and F1 on weak labels; threshold tuned by F1.\n",
      "- **Task 2 (Service Similarity)**: cosine NN and KMeans; high silhouette with compact role vectors.\n",
      "- **Watchlist**: ranked by anomaly impact × cost; includes latest month KPIs.\n",
      "\n",
      "Artifacts: `task1_results_variants.csv`, `task1_top_incidents.csv`, `task2_neighbors.csv`, `task2_clustering_results.csv`, `watchlist_apps.csv`.\n",
      "\n",
      "## Task 1 — Insight Detection (Naive Bayes Variants)\n",
      "\n",
      "| Feature Set / Variant | Precision | Recall | F1 | PR-AUC | Accuracy (+ve) | Accuracy (mean) | Threshold | Valid size | Pos rate (valid) |\n",
      "|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "| Counts only (NB) | 0.10568402532383772 | 0.8179001721170396 | 0.1830056388612474 | 0.1021308496893949 | 0.9179575444635686 | 0.3043420390540036 | 0.3514217132994965 | 308035 | 0.0848767185547097 |\n",
      "| All features (NB) | 0.9028533849694933 | 0.9451902849493211 | 0.9235196292766784 | 0.9774733165108653 | 0.9451902849493211 | 0.9867125488986641 | 0.9806067807015026 | 308035 | 0.0848767185547097 |\n",
      "| PCA(3) + NB | 0.6810904129423886 | 0.9355517307324537 | 0.7882821186290465 | 0.6950759672884285 | 0.9355517307324537 | 0.9573457561640722 | 0.154903618666408 | 308035 | 0.0848767185547097 |\n",
      "\n",
      "## Task 2 — Service Similarity (Top-5 Neighbors, sample)\n",
      "\n",
      "| service | n1 | s1 | n2 | s2 | n3 | s3 | n4 | s4 | n5 | s5 |\n",
      "|---|---|---:|---|---:|---|---:|---|---:|---|---:|\n",
      "| ABTEST | NOTIFY | 0.9999999999809024 | MKTDB | 0.9999999999349333 | DYNPRC | 0.9999999998786389 | XCHATTR | 0.9999999997867001 | TAXCALC | 0.9999999996770227 |\n",
      "| AIRFLW | HMFRP | 0.999999998609385 | YELLOWS | 0.9999999953721767 | KAFKA | 0.9999999888122127 | DDSD | 0.9999999793005887 | ELASTIC | 0.9999996950612356 |\n",
      "| ANALAPI | AUTOML | 0.9999992614814076 | RISKMG | 0.9999991143549807 | BILLING | 0.9999989627174771 | GLOBCMP | 0.9999989224119094 | SUPCHN | 0.9999988448202543 |\n",
      "| APIGWY | SNAPINT | 0.9999999992863867 | MULTILNG | 0.9999999971777137 | SLSDB | 0.9999999962098263 | TAXCALC | 0.9999999958172383 | XCHATTR | 0.9999999955270946 |\n",
      "| ATLAS | DATAHUB | 0.9999896440875172 | MDLREG | 0.9999718818442569 | JPNXPP | 0.999951212040954 | SPARK | 0.9999474668239722 | PANDRA | 0.9999318254451391 |\n",
      "| AUTOML | RISKMG | 0.9999999930268135 | BILLING | 0.9999999744404572 | GLOBCMP | 0.9999999675906348 | SUPCHN | 0.9999999532220397 | PAYPROC | 0.9999998582373713 |\n",
      "| AWS | AZURE | 0.9999999999941895 | SNWFLK | 0.9999999999270122 | DBRCKS | 0.9999999998043644 | HMFRP | 0.7071067811150845 | AIRFLW | 0.7071067796016148 |\n",
      "| AZURE | AWS | 0.9999999999941895 | SNWFLK | 0.9999999999622053 | DBRCKS | 0.9999999998659184 | HMFRP | 0.7071067811452451 | AIRFLW | 0.7071067797588978 |\n",
      "| BILLING | GLOBCMP | 0.999999999573766 | SUPCHN | 0.9999999968161862 | RISKMG | 0.9999999940065972 | AUTOML | 0.9999999744404572 | PAYPROC | 0.9999999528774104 |\n"
     ]
    }
   ],
   "source": [
    "# Build a concise, slide-ready report: report_pack.md\n",
    "import csv, os\n",
    "\n",
    "t1_var_csv   = \"task1_results_variants.csv\"\n",
    "t2_nbr_csv   = \"task2_neighbors.csv\"\n",
    "t2_clu_csv   = \"task2_clustering_results.csv\"\n",
    "inc_md       = \"task1_incident_summaries.md\"\n",
    "watch_csv    = \"watchlist_apps.csv\"\n",
    "\n",
    "out_md = \"report_pack.md\"\n",
    "lines = []\n",
    "\n",
    "lines += [\n",
    "    \"# Trufflow 1B — Data Product Insight Recommendation\",\n",
    "    \"\",\n",
    "    \"## Summary\",\n",
    "    \"- **Task 1 (Insight Detection, NB)**: strong PR-AUC and F1 on weak labels; threshold tuned by F1.\",\n",
    "    \"- **Task 2 (Service Similarity)**: cosine NN and KMeans; high silhouette with compact role vectors.\",\n",
    "    \"- **Watchlist**: ranked by anomaly impact × cost; includes latest month KPIs.\",\n",
    "    \"\",\n",
    "    \"Artifacts: `task1_results_variants.csv`, `task1_top_incidents.csv`, `task2_neighbors.csv`, `task2_clustering_results.csv`, `watchlist_apps.csv`.\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "# Task 1 table\n",
    "if os.path.exists(t1_var_csv):\n",
    "    with open(t1_var_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "    hdr = [\"Feature Set / Variant\",\"Precision\",\"Recall\",\"F1\",\"PR-AUC\",\"Accuracy (+ve)\",\"Accuracy (mean)\",\"Threshold\",\"Valid size\",\"Pos rate (valid)\"]\n",
    "    lines += [\"## Task 1 — Insight Detection (Naive Bayes Variants)\", \"\"]\n",
    "    lines += [\"| \" + \" | \".join(hdr) + \" |\"]\n",
    "    lines += [\"|\" + \"|\".join([\"---\"] + [\":---:\" for _ in hdr[1:]]) + \"|\"]\n",
    "    for r in rows:\n",
    "        lines += [\"| \" + \" | \".join(str(r[h]) for h in hdr) + \" |\"]\n",
    "    lines += [\"\"]\n",
    "\n",
    "# Task 2 neighbors (sample)\n",
    "if os.path.exists(t2_nbr_csv):\n",
    "    with open(t2_nbr_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        nbr_rows = list(csv.DictReader(f))[:10]\n",
    "    lines += [\"## Task 2 — Service Similarity (Top-5 Neighbors, sample)\", \"\"]\n",
    "    hdr = [\"service\",\"n1\",\"s1\",\"n2\",\"s2\",\"n3\",\"s3\",\"n4\",\"s4\",\"n5\",\"s5\"]\n",
    "    lines += [\"| \" + \" | \".join(hdr) + \" |\"]\n",
    "    lines += [\"|---|---|---:|---|---:|---|---:|---|---:|---|---:|\"]\n",
    "    for r in nbr_rows:\n",
    "        lines += [\"| \" + \" | \".join(r[h] for h in hdr) + \" |\"]\n",
    "    lines += [\"\"]\n",
    "\n",
    "# Task 2 clustering summary\n",
    "if os.path.exists(t2_clu_csv):\n",
    "    with open(t2_clu_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        clu_rows = list(csv.DictReader(f))\n",
    "    lines += [\"## Task 2 — Clustering Summary (Silhouette, cosine)\", \"\"]\n",
    "    lines += [\"| Representation | Method | k | Silhouette |\"]\n",
    "    lines += [\"|---|---|---:|---:|\"]\n",
    "    for r in clu_rows:\n",
    "        lines += [f\"| {r['Representation']} | {r['Method']} | {r['k']} | {r['Silhouette']} |\"]\n",
    "    lines += [\"\"]\n",
    "\n",
    "# Watchlist Top-20\n",
    "if os.path.exists(watch_csv):\n",
    "    with open(watch_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "        wrows = list(csv.DictReader(f))[:20]\n",
    "    lines += [\"## Watchlist — Top 20 Apps (by anomaly impact × cost)\", \"\"]\n",
    "    hdr = [\"app_id\",\"anomalies_total\",\"score_total\",\"latest_month\",\"cost_latest\",\"cost_mom_pct\",\"value_per_cost_latest\",\"outage_count_latest\",\"outage_duration_latest\"]\n",
    "    lines += [\"| \" + \" | \".join(hdr) + \" |\"]\n",
    "    lines += [\"|---|---:|---:|---|---:|---:|---:|---:|---:|\"]\n",
    "    for r in wrows:\n",
    "        lines += [\"| \" + \" | \".join(str(r.get(h, \"\")) for h in hdr) + \" |\"]\n",
    "    lines += [\"\"]\n",
    "\n",
    "# Top incident summaries (sample)\n",
    "if os.path.exists(inc_md):\n",
    "    lines += [\"## Top Incidents — Brief Summaries (sample)\", \"\"]\n",
    "    with open(inc_md, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in zip(range(40), f):  # ~first 40 lines\n",
    "            lines.append(line.rstrip())\n",
    "    lines += [\"\", f\"_See full: `{inc_md}`_\"]\n",
    "\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Saved:\", out_md)\n",
    "with open(out_md, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in zip(range(30), f):\n",
    "        print(line.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8bffcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib unavailable — saved curve points to task1_pr_curve.csv\n",
      "{'avg_precision': 0.9774733165108653, 'best_F1': 0.9235368856651477, 'best_point': {'precision': 0.9028533849694933, 'recall': 0.9451902849493211}, 'best_threshold': 0.9806067807015026}\n"
     ]
    }
   ],
   "source": [
    "# PR curve for Task 1 (uses X, keys from earlier)\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, f1_score\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# Rebuild weak labels\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "y = ((er > 0.10) | (robust_z(cmean) > 3.0) | (robust_z(dmean) > 3.0)).astype(int)\n",
    "\n",
    "# Time-based split\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "# Train NB\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]).fit(Xtr, ytr)\n",
    "\n",
    "# PR data\n",
    "proba = model.predict_proba(Xva)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "bi = int(np.argmax(f1s))\n",
    "best_thr = thr[bi-1] if bi>0 and (bi-1)<len(thr) else 0.5\n",
    "best_f1 = float(f1s[bi])\n",
    "ap = float(average_precision_score(yva, proba))\n",
    "\n",
    "# Try to plot; if matplotlib unavailable, save CSV instead\n",
    "plotted = False\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f\"PR curve (AP={ap:.3f})\")\n",
    "    # Mark best F1 point\n",
    "    plt.scatter([rec[bi]], [prec[bi]], s=40)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Task 1 — Precision–Recall\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"task1_pr_curve.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"Saved figure: task1_pr_curve.png\")\n",
    "    plotted = True\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "if not plotted:\n",
    "    import csv\n",
    "    with open(\"task1_pr_curve.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f); w.writerow([\"recall\",\"precision\"])\n",
    "        for r_, p_ in zip(rec, prec): w.writerow([float(r_), float(p_)])\n",
    "    print(\"matplotlib unavailable — saved curve points to task1_pr_curve.csv\")\n",
    "\n",
    "print({\n",
    "    \"avg_precision\": ap,\n",
    "    \"best_F1\": best_f1,\n",
    "    \"best_point\": {\"precision\": float(prec[bi]), \"recall\": float(rec[bi])},\n",
    "    \"best_threshold\": float(best_thr)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8be492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION (labels=[0,1]):\n",
      "[[279230   2660]\n",
      " [  1433  24712]]\n",
      "\n",
      "METRICS:\n",
      "{'threshold': 0.9806067807015026, 'avg_precision': 0.9774733165108653, 'accuracy': 0.9867125488986641, 'precision': 0.9028204004091772, 'recall': 0.9451902849493211, 'f1': 0.9235196292766784, 'pos_rate_valid': 0.0848767185547097, 'pred_pos_rate': 0.08886003213920496, 'TN': 279230, 'FP': 2660, 'FN': 1433, 'TP': 24712}\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9949    0.9906    0.9927    281890\n",
      "           1     0.9028    0.9452    0.9235     26145\n",
      "\n",
      "    accuracy                         0.9867    308035\n",
      "   macro avg     0.9489    0.9679    0.9581    308035\n",
      "weighted avg     0.9871    0.9867    0.9869    308035\n",
      "\n",
      "\n",
      "Saved: task1_confusion_matrix.csv, task1_confusion_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix + classification report at best-F1 threshold (Task 1)\n",
    "import numpy as np, csv, json\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "assert 'X' in globals() and 'keys' in globals(), \"Run the feature cell first.\"\n",
    "\n",
    "# Rebuild weak labels (same rule used before)\n",
    "er = X[:,1]; cmean = X[:,3]; dmean = X[:,5]\n",
    "def robust_z(x):\n",
    "    med = np.median(x); mad = np.median(np.abs(x - med)) + 1e-9\n",
    "    return np.abs(x - med) / (1.4826 * mad)\n",
    "y = ((er > 0.10) | (robust_z(cmean) > 3.0) | (robust_z(dmean) > 3.0)).astype(int)\n",
    "\n",
    "# Time-based split\n",
    "idx = np.arange(len(keys))\n",
    "idx = idx[np.argsort([k[2] for k in keys])]\n",
    "cut = int(len(idx)*0.8)\n",
    "train_idx, valid_idx = idx[:cut], idx[cut:]\n",
    "Xtr, ytr = X[train_idx], y[train_idx]\n",
    "Xva, yva = X[valid_idx], y[valid_idx]\n",
    "\n",
    "# Train & score\n",
    "model = Pipeline([(\"scaler\", StandardScaler()), (\"nb\", GaussianNB())]).fit(Xtr, ytr)\n",
    "proba = model.predict_proba(Xva)[:,1]\n",
    "\n",
    "# Best-F1 threshold (same as PR cell)\n",
    "prec, rec, thr = precision_recall_curve(yva, proba)\n",
    "f1s = (2*prec*rec)/(prec+rec+1e-9)\n",
    "bi = int(np.argmax(f1s))\n",
    "best_thr = thr[bi-1] if bi>0 and (bi-1)<len(thr) else 0.5\n",
    "\n",
    "# Predictions at chosen threshold\n",
    "yhat = (proba >= best_thr).astype(int)\n",
    "\n",
    "# Confusion matrix + metrics\n",
    "cm = confusion_matrix(yva, yhat, labels=[0,1])\n",
    "TN, FP, FN, TP = int(cm[0,0]), int(cm[0,1]), int(cm[1,0]), int(cm[1,1])\n",
    "metrics = {\n",
    "    \"threshold\": float(best_thr),\n",
    "    \"avg_precision\": float(average_precision_score(yva, proba)),\n",
    "    \"accuracy\": float(accuracy_score(yva, yhat)),\n",
    "    \"precision\": float(precision_score(yva, yhat, zero_division=0)),\n",
    "    \"recall\": float(recall_score(yva, yhat, zero_division=0)),\n",
    "    \"f1\": float(f1_score(yva, yhat, zero_division=0)),\n",
    "    \"pos_rate_valid\": float(yva.mean()),\n",
    "    \"pred_pos_rate\": float(yhat.mean()),\n",
    "    \"TN\": TN, \"FP\": FP, \"FN\": FN, \"TP\": TP\n",
    "}\n",
    "\n",
    "print(\"CONFUSION (labels=[0,1]):\")\n",
    "print(cm)\n",
    "print(\"\\nMETRICS:\")\n",
    "print(metrics)\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT:\")\n",
    "print(classification_report(yva, yhat, digits=4))\n",
    "\n",
    "# Save matrix + summary\n",
    "with open(\"task1_confusion_matrix.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"\", \"Pred_0\", \"Pred_1\"])\n",
    "    w.writerow([\"Actual_0\", TN, FP])\n",
    "    w.writerow([\"Actual_1\", FN, TP])\n",
    "\n",
    "with open(\"task1_confusion_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved: task1_confusion_matrix.csv, task1_confusion_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55631658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: milestone1_key_tables.md\n",
      "# Key Results — Milestone 1\n",
      "\n",
      "## Transactions — Coverage\n",
      "| Item | Value |\n",
      "|---|---:|\n",
      "| Transactions rows read | **7,254,656** |\n",
      "| Aggregated hourly windows | **1,540,175** |\n",
      "| Feature matrix shape | **(1,540,175, 6)** |\n",
      "\n",
      "## Data & Feature Engineering (Daily/Monthly)\n",
      "| Item | Value |\n",
      "|---|---:|\n",
      "| Unique services represented | **87** |\n",
      "| Daily metrics rows / days | **666,855 / 365** |\n",
      "| Monthly metrics rows / months | **18,270 / 10** |\n",
      "\n",
      "## Task 1 — Naive Bayes (baseline + variants)\n",
      "| Feature Set / Variant | Precision | Recall | F1 | PR-AUC | Accuracy (mean) | Threshold | Valid size | Pos rate (valid) |\n",
      "|---|---:|---:|---:|---:|---:|---:|---:|---:|\n",
      "| Counts only (NB) | 0.1057 | 0.8179 | 0.183 | 0.1021 | 0.3043 | 0.3514 | 308,035 | 0.0849 |\n",
      "| All features (NB) | 0.9029 | 0.9452 | 0.9235 | 0.9775 | 0.9867 | 0.9806 | 308,035 | 0.0849 |\n",
      "| PCA(3) + NB | 0.6811 | 0.9356 | 0.7883 | 0.6951 | 0.9573 | 0.1549 | 308,035 | 0.0849 |\n",
      "\n",
      "## Task 1 — PR/Threshold & Confusion (at best-F1 threshold)\n",
      "| Metric | Value |\n",
      "|---|---:|\n",
      "| Best threshold (by F1) | **0.9806** |\n",
      "| Average precision (PR-AUC) | **0.9775** |\n",
      "| Accuracy (mean) | **0.9867** |\n",
      "| Precision / Recall | **0.9028 / 0.9452** |\n",
      "| F1 | **0.9235** |\n",
      "| Validation positive rate | **0.0849** |\n",
      "| Predicted positive rate | **0.0889** |\n",
      "| Confusion matrix (valid) | **TN=279,230  FP=2,660  FN=1,433  TP=24,712** |\n",
      "\n",
      "## Task 1 — Top Incidents (from transactions, Top 10)\n",
      "| time_bucket | consumer_id | supplier_id | probability | predicted_anomaly | weak_label | req_count | error_rate | cost_sum | cost_mean | data_sum | data_mean |\n",
      "|---|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\n",
      "| 2025-05-12 02:00:00 | ECOMLP | MCSCBT | 1 | 1 | 1 | 6 | 0 | 3,539.95 | 589.99 | 70.8 | 11.8 |\n",
      "| 2025-03-30 01:00:00 | CUSTPRT | MCSCBT | 1 | 1 | 1 | 6 | 0 | 3,465.25 | 577.54 | 73.05 | 12.18 |\n"
     ]
    }
   ],
   "source": [
    "# Milestone1_key_tables.md \n",
    "import os, csv, json\n",
    "\n",
    "out_md = \"milestone1_key_tables.md\"\n",
    "\n",
    "def fmt(x, nd=4):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        if abs(f) >= 1000:\n",
    "            return f\"{f:,.{nd}f}\".rstrip(\"0\").rstrip(\".\")\n",
    "        return f\"{f:.{nd}f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "# ---------- Transactions counts ----------\n",
    "tx_rows = \"N/A\"\n",
    "if os.path.exists(\"transactions.jsonl\"):\n",
    "    try:\n",
    "        with open(\"transactions.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            tx_rows = sum(1 for _ in f)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# From memory if available\n",
    "agg_windows = None\n",
    "feat_shape = None\n",
    "try:\n",
    "    agg_windows = len(keys)\n",
    "    feat_shape = f\"({X.shape[0]:,}, {X.shape[1]})\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ---------- Services count from neighbors ----------\n",
    "services_count = \"N/A\"\n",
    "if os.path.exists(\"task2_neighbors.csv\"):\n",
    "    with open(\"task2_neighbors.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        services_count = f\"{sum(1 for _ in csv.DictReader(f)):,}\"\n",
    "\n",
    "# ---------- Daily & monthly counts ----------\n",
    "def count_lines(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return sum(1 for _ in f)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "daily_rows = count_lines(\"daily_metrics.jsonl\")\n",
    "monthly_rows = count_lines(\"monthly_metrics.jsonl\")\n",
    "\n",
    "days_unique = \"N/A\"\n",
    "if os.path.exists(\"daily_features.csv\"):\n",
    "    seen = set()\n",
    "    with open(\"daily_features.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for r in csv.DictReader(f): seen.add(r[\"day\"])\n",
    "    days_unique = f\"{len(seen):,}\"\n",
    "\n",
    "months_unique = \"N/A\"\n",
    "if os.path.exists(\"monthly_features.csv\"):\n",
    "    seen = set()\n",
    "    with open(\"monthly_features.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for r in csv.DictReader(f): seen.add(r[\"month\"])\n",
    "    months_unique = f\"{len(seen):,}\"\n",
    "\n",
    "# ---------- Task 1 variants ----------\n",
    "t1_rows = []\n",
    "if os.path.exists(\"task1_results_variants.csv\"):\n",
    "    with open(\"task1_results_variants.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        t1_rows = list(csv.DictReader(f))\n",
    "\n",
    "# ---------- Task 1 confusion summary ----------\n",
    "conf = {}\n",
    "if os.path.exists(\"task1_confusion_summary.json\"):\n",
    "    with open(\"task1_confusion_summary.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        conf = json.load(f)\n",
    "\n",
    "# ---------- Task 1 top incidents (from transactions)\n",
    "top_inc = []\n",
    "if os.path.exists(\"task1_top_incidents.csv\"):\n",
    "    with open(\"task1_top_incidents.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        top_inc = list(csv.DictReader(f))[:10]\n",
    "\n",
    "# ---------- Task 2 clustering sweep & neighbors ----------\n",
    "clu_rows = []\n",
    "best_k, best_sil = None, None\n",
    "if os.path.exists(\"task2_clustering_results.csv\"):\n",
    "    with open(\"task2_clustering_results.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        clu_rows = list(csv.DictReader(f))\n",
    "    if clu_rows:\n",
    "        best = max(clu_rows, key=lambda r: float(r[\"Silhouette\"]))\n",
    "        best_k, best_sil = best[\"k\"], best[\"Silhouette\"]\n",
    "\n",
    "nbr_rows = []\n",
    "if os.path.exists(\"task2_neighbors.csv\"):\n",
    "    with open(\"task2_neighbors.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        nbr_rows = list(csv.DictReader(f))[:15]\n",
    "\n",
    "# ---------- Watchlist top 5 ----------\n",
    "watch_top = []\n",
    "if os.path.exists(\"watchlist_apps.csv\"):\n",
    "    with open(\"watchlist_apps.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "        watch_top = list(csv.DictReader(f))[:5]\n",
    "\n",
    "# ---------------- Compose Markdown ----------------\n",
    "lines = []\n",
    "lines.append(\"# Key Results — Milestone 1\\n\")\n",
    "\n",
    "# Transactions (explicit)\n",
    "lines += [\n",
    "\"## Transactions — Coverage\",\n",
    "\"| Item | Value |\",\n",
    "\"|---|---:|\",\n",
    "f\"| Transactions rows read | **{(tx_rows if isinstance(tx_rows,int) else tx_rows):,}** |\" if isinstance(tx_rows, int) else f\"| Transactions rows read | **{tx_rows}** |\",\n",
    "f\"| Aggregated hourly windows | **{agg_windows:,}** |\" if isinstance(agg_windows, int) else \"| Aggregated hourly windows | **N/A** |\",\n",
    "f\"| Feature matrix shape | **{feat_shape or 'N/A'}** |\",\n",
    "\"\"\n",
    "]\n",
    "\n",
    "# Data & feature engineering (daily/monthly + services)\n",
    "lines += [\n",
    "\"## Data & Feature Engineering (Daily/Monthly)\",\n",
    "\"| Item | Value |\",\n",
    "\"|---|---:|\",\n",
    "f\"| Unique services represented | **{services_count}** |\",\n",
    "f\"| Daily metrics rows / days | **{(daily_rows and format(daily_rows, ',') or 'N/A')} / {days_unique}** |\",\n",
    "f\"| Monthly metrics rows / months | **{(monthly_rows and format(monthly_rows, ',') or 'N/A')} / {months_unique}** |\",\n",
    "\"\"\n",
    "]\n",
    "\n",
    "# Task 1 variants\n",
    "lines += [\n",
    "\"## Task 1 — Naive Bayes (baseline + variants)\",\n",
    "\"| Feature Set / Variant | Precision | Recall | F1 | PR-AUC | Accuracy (mean) | Threshold | Valid size | Pos rate (valid) |\",\n",
    "\"|---|---:|---:|---:|---:|---:|---:|---:|---:|\",\n",
    "]\n",
    "for r in t1_rows:\n",
    "    lines.append(\n",
    "        f\"| {r['Feature Set / Variant']} | {fmt(r['Precision'])} | {fmt(r['Recall'])} | {fmt(r['F1'])} | {fmt(r['PR-AUC'])} | \"\n",
    "        f\"{fmt(r['Accuracy (mean)'])} | {fmt(r['Threshold'])} | {int(float(r['Valid size'])):,} | {fmt(r['Pos rate (valid)'])} |\"\n",
    "    )\n",
    "lines.append(\"\")\n",
    "\n",
    "# Task 1 PR/confusion\n",
    "if conf:\n",
    "    lines += [\n",
    "    \"## Task 1 — PR/Threshold & Confusion (at best-F1 threshold)\",\n",
    "    \"| Metric | Value |\",\n",
    "    \"|---|---:|\",\n",
    "    f\"| Best threshold (by F1) | **{fmt(conf.get('threshold'))}** |\",\n",
    "    f\"| Average precision (PR-AUC) | **{fmt(conf.get('avg_precision'))}** |\",\n",
    "    f\"| Accuracy (mean) | **{fmt(conf.get('accuracy'))}** |\",\n",
    "    f\"| Precision / Recall | **{fmt(conf.get('precision'))} / {fmt(conf.get('recall'))}** |\",\n",
    "    f\"| F1 | **{fmt(conf.get('f1'))}** |\",\n",
    "    f\"| Validation positive rate | **{fmt(conf.get('pos_rate_valid'))}** |\",\n",
    "    f\"| Predicted positive rate | **{fmt(conf.get('pred_pos_rate'))}** |\",\n",
    "    f\"| Confusion matrix (valid) | **TN={int(conf.get('TN',0)):,}  FP={int(conf.get('FP',0)):,}  FN={int(conf.get('FN',0)):,}  TP={int(conf.get('TP',0)):,}** |\",\n",
    "    \"\"\n",
    "    ]\n",
    "\n",
    "# Task 1 Top Incidents (transactions-derived)\n",
    "if top_inc:\n",
    "    lines += [\n",
    "    \"## Task 1 — Top Incidents (from transactions, Top 10)\",\n",
    "    \"| time_bucket | consumer_id | supplier_id | probability | predicted_anomaly | weak_label | req_count | error_rate | cost_sum | cost_mean | data_sum | data_mean |\",\n",
    "    \"|---|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|\",\n",
    "    ]\n",
    "    for r in top_inc:\n",
    "        lines.append(\n",
    "            f\"| {r['time_bucket']} | {r['consumer_id']} | {r['supplier_id']} | {fmt(r['probability'],3)} | {r['predicted_anomaly']} | {r['weak_label']} | \"\n",
    "            f\"{fmt(r['req_count'],0)} | {fmt(r['error_rate'],3)} | {fmt(r['cost_sum'],2)} | {fmt(r['cost_mean'],2)} | {fmt(r['data_sum'],2)} | {fmt(r['data_mean'],2)} |\"\n",
    "        )\n",
    "    lines.append(\"\")\n",
    "\n",
    "# Task 2 summary\n",
    "lines += [\n",
    "\"## Task 2 — Service Similarity & Clustering (Cosine)\",\n",
    "\"| Item | Value |\",\n",
    "\"|---|---:|\",\n",
    "f\"| Services (vectors) | **{services_count}** |\",\n",
    "\"| Role vector dimensionality | **8** |\",\n",
    "f\"| Best k (KMeans, cosine) | **{best_k or 'N/A'}** |\",\n",
    "f\"| Silhouette (cosine) @ best k | **{fmt(best_sil) if best_sil else 'N/A'}** |\",\n",
    "\"\"\n",
    "]\n",
    "\n",
    "# Task 2 clustering sweep (full)\n",
    "if clu_rows:\n",
    "    lines += [\n",
    "    \"### Clustering Sweep (Silhouette over k)\",\n",
    "    \"| k | Silhouette (cosine) |\",\n",
    "    \"|---:|---:|\",\n",
    "    ]\n",
    "    for r in clu_rows:\n",
    "        lines.append(f\"| {r['k']} | {fmt(r['Silhouette'])} |\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "# Task 2 nearest neighbors (sample)\n",
    "if nbr_rows:\n",
    "    lines += [\n",
    "    \"### Nearest Neighbors (Top-5 per service, sample of 15)\",\n",
    "    \"| service | n1 | s1 | n2 | s2 | n3 | s3 | n4 | s4 | n5 | s5 |\",\n",
    "    \"|---|---|---:|---|---:|---|---:|---|---:|---|---:|\",\n",
    "    ]\n",
    "    for r in nbr_rows:\n",
    "        lines.append(\"| \" + \" | \".join([r[h] for h in [\"service\",\"n1\",\"s1\",\"n2\",\"s2\",\"n3\",\"s3\",\"n4\",\"s4\",\"n5\",\"s5\"]]) + \" |\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "# Watchlist Top-5\n",
    "if watch_top:\n",
    "    lines += [\n",
    "    \"## Watchlist — Top 5 Apps by Anomaly Impact × Cost (validation slice)\",\n",
    "    \"| # | App | Anomalies (total) | Score_total | Latest month | Cost_latest | MoM cost % | Value/Cost (latest) | Outages (count / duration) |\",\n",
    "    \"|---:|---|---:|---:|---|---:|---:|---:|---:|\",\n",
    "    ]\n",
    "    for i, r in enumerate(watch_top, 1):\n",
    "        lines.append(\n",
    "            f\"| {i} | {r['app_id']} | {int(float(r['anomalies_total'])):,} | {fmt(r['score_total'],2)} | {r['latest_month']} | \"\n",
    "            f\"{fmt(r.get('cost_latest',''),2)} | {fmt(r.get('cost_mom_pct',''))} | {fmt(r.get('value_per_cost_latest',''),5)} | \"\n",
    "            f\"{int(float(r.get('outage_count_latest',0))):,} / {int(float(r.get('outage_duration_latest',0))):,} |\"\n",
    "        )\n",
    "    lines.append(\"\")\n",
    "\n",
    "# Write + preview\n",
    "with open(out_md, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Saved:\", out_md)\n",
    "with open(out_md, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in zip(range(40), f):\n",
    "        print(line.rstrip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f654642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
